大家好
我是东北大学数据挖掘课题组的王培东
欢迎大家来到第四期书生大模型实战营
恭喜各位来到进阶关卡
今天要介绍的是多模态大模型发展简述
及其微调部署时间
在实战部分
会以上海人工智能实验室研发的
性能强劲的多模态大模型
以ten vl2为例进行操作
首先呢我们会介绍一下多模态大模型
相关的基础知识
方便大家简要了解前人的工作
快速入门
动模态大模型
对多模态大模型有一个总体的认知
第二部分
我们会一起学习一下INPVL2的模型架构
以及其训练方法
让大家对我们即将实战体验的模型
有一个大概的认知
最后一部分就是利用上海人工智能实验室
发布的两款开源工具l m deploy和XTUNNER
进行微部署和微调的实战
什么是多模态大模型呢
其实更准确的讲是多模态大语言模型
因为现在多数都是以大语言模型为基座
进行开发的
Multi model language
Large language model
是指能够处理和融合多种不同类型数据的
大型人工智能模型
这里的多模态指的
就是多种不同类型的数据模态
其实就是一种信息的载体
比方说我现在说话的声音
就是我向你们传递信息的一种载体
它可以被认为是一种模态
你们在PPT中看到的文本和图像
也都是不同的信息载体
因此也是两种模态
所以多模态所做的就是要研究
如何恰当的综合处理不同模态的信息
这里列出了几个常见的多模态大模型
比如国产开源的比较好的in ten vl
千万VL嗯
国外闭源的GPT4O和开源的拉瓦
相信大家多多少少都听说过这几个模型的名字
这页列出了使用多模态大模型的例子
左边是利用呃intend VR分析图像的信息
比如说他这里问图中展示的是什么物体嗯
INTEVL就可以呃比较详尽
比较科学的给这个图片做一个描述
下面用户继续询问这些图片相关的一些信息
这个INTEVL2也能继续呃
根据用户的意图去回答他所他想知道的信息
右边这个是用DV4来解释一
这张图像笑点在哪呃
可以看出他这个图像是一个呃
手机插了一个VGA的接口的线
它下面那个GB4都能呃
比较好的解释出这个笑点在哪儿
可以看出工作中
生活中到处都有做动态大模型的用武之地
根据前面的介绍
聪明的你可能已经猜到了多模态
大模型乃至多模态这个研究方向的重点了
通俗点说就是如何融合各种模态的信息
学术一点讲就是如何对齐不同模态的特征空间
解释一下不同模态的信息
其编码后的表征不同
这些不同模态的数据
通常是由不同的模块来进行编码的
因此得到的特征向量的表征空间不同
对于同一语义在不同特征空间中
其表示不同
因此需要一些设计来弥补这个语义gap
就是要想方设法对齐不同模态之间的特征空间
继续跟我学习
在后面的介绍中
你会对这句话有更深的体会
先介绍一下blip two
这是去年1月份的工作
算是多模态大模型领域
最早最有影响力的工作之一
嗯对于输入的图像
经过图像编码器的编码得到图像特征
然后通过一系列的模块的变换
将图像特征投影至文本空间
送入LLM
因为LLM处理的是文本特征
所以想让LLM获得视觉能力
就需要把视觉信息对齐到文本空间中
这样LLM就可以综合处理图像和文本的信息
这里着重介绍一下上一页所说的一些模块
就是QFORMWORK
可以看一下它的架构
其实就是一些堆叠的transform模块
有些类似之前经典的多模态模型的
双塔设计结构
先看左塔
最底下是一个可学习的query
这个query呢就是一些可学习的向量嗯
这个query的作用就是
通过上面这个across attention
把图像中的重点信息给查询出来
起到抽取图像图像中关键信息的作用
右边这个塔呢是编码文本用的
可以注意到这两个塔之间
通过self attention来进行参数的共享
起到一定的模态融合的作用
后面的is it for word层
就是FMN层
它不共享参数呃
类似于MOE中的那个专家模块
处理模态的差异化信息
q former呢学习三个loss
第一个呢是图文匹配loss呃
第二个是基于图像的文本生成loss
以及图文的对比学习loss
通过这三个图文任务来优化
这个多模态模型的对齐效果
这三个loss由于它任务不同
所以需要不同的attention mask
就如右图所示呃
对于这个图文匹配任务
图像测需要完整的看到文本
文本测也需要完整的看到图像
所以两边都有mask
都是公开透明的
全是白块嗯
对于这个基于图像的文本生成
这个任务需要文本predict next token
所以他不能看到自己后续的呃一些token
然后图像这段也根本不需要看到文本
因为它是做文本生成的啊
所以说呃这边全部mask掉
然后文本这边是一个下三角矩阵
对于对比学习任务
由于它只是图像自己编码
自己文本自己编码文本
所以它只需要看到自己就行了
把对方的那个模态mask掉了
通过对q former的单独训练
和后面与LLM主干的联合训练
可以学到一个很好的特征
投影器
使得LLM可以融合处理图文信息
嗯这页展示呢是去年爆火的mini g b t four呃
它就类似于社区副线的开源版GB1F
它采用的就是呃q former呃
他所他所采用的就是brief to所提出的q former
作为它的对齐模块嗯
后面还有个线性层
这个线性层是为了调整维度用的
把conformer输出的维度跟LLM需要的维度呃
对齐到一块儿呃
larva这篇开拓了一个另外的设计方式
它的设计模式比较简单
就是用一个简单的线性层
把图像特征投影到文本空间
虽然它的设计模式很简单呃
需要训练的参数量很小
但是它效果又特别的不错
后面的很多工作都是follow这个做法做的
这篇文章的名字也很简单粗暴
就叫visual instruct tuning呃
视觉的指令调优嗯
他就是把呃给定一个输入图像
然后经过图像编码器编码之后呃
通过一个简单的线性层
就一层线性层进行投影
得到的特征跟呃文本指令的特征呃
呃embedding拼在一起
送入大模型
然后让大模型做一些呃视觉指令
任务就大概就是这样
我们知道一个训练好的图像编码器
它的分辨率一般是固定的
处理其他分辨率的图像时
常见的处理策略是直接reshape过去
但是如果这样做的话
就会丢失原始图像含有的细粒度信息
Lava1.5 h d
它就引入了一种处理更大分辨率图像
的一个新的范式
就是把一个图像进行切片
切到图像编码器能够处理的分辨率之后
分别对这些呃块进行编码
同时为了提供全局信息
将原图像reset后呃
再送进去编码
这个做法看上去很符合直觉
图像太大就切成小的处理
最后还要有个缩略图
把握整体的信息
实际上最后效果也很棒
嗯对了
这个对齐模块从lava的一个单一的线性层
一层线性层替换成了一个双层的MLP
lava nex是larva1.5HD的升级版
它采用了动态的分辨率策略
这样就可以处理多种分辨率的图像
它预设了几种长宽比
看输入图像跟哪个长宽比
最近似就放缩过去
然后切块
他还利用了更优质的训练数据去做训练
最近的一些工作
大家都在使用lava next的模型架构
就是在训练数据上花一些自己的功夫
这个可以说是模型设计架构的一个暂时的高峰
目前许多强大的开源模型
都是采用类似的这个设计方案
目前大家似乎都倾向于lava这种VITM
LPLLM的设计方式
而使用q former做对齐的却少
这里我总结了几点可能的原因
第一个就是q former的收敛速度慢
刚刚也介绍了q former是N层的transformer块
它的参数量自然要比MLP要大好多
以blip去为例
它的q former中有100M的参数
就是呃一百百万一亿的参数
如果是MLP就远远没有这么多参数
也就几M就差不多够了
而且performer的收敛效率也有问题
在同样的计算量的情况下
MLP可以更快的收敛
第二点呢就是CUFORMER
即使带来了更大的计算量
但是它并没有体现出更大的优势
在充足的计算量的前提下
性能提升也不显著
无法超越简单的MLP方案
第三点就是AVA的后续工作
lava1.5的性能强劲
成为了一个强大的baseline
而blip的后续工作
instruct blip并没有带来显著的性能提升
且无法推广至多轮对话
第四点
lava的结构简洁又高效
可用性强
大家很容易就把lava那套训练代码拿去呃
自己改造
然后去训练自己的模型呃
这也是它的一大优势
至此多模态大模型基础知识部分已经介绍完了
恭喜你学到这里
可以先去休息一下
一会儿回来咱们一起学习INTEVL2的模型架构
in turn vl2的模型架构是怎么设计的呢
它采取的是拉瓦式的架构设计
即VITMLP嗯
LLM它的基座大模型选用的是INTELM2
视觉编码器
选用的是他们自己训练的intend v i t
6B对其模块选用的是MLP
下面我会稍微深入一下介绍各个模块嗯
in turn v i t它的结构也是一个VIT
可能一些小组件有些创新
但大差不差
左上角这个图是intend vl的训练流程
可以看出intend vl的训练流程
它与之前通过监督学习
或者是clip那种对比学习训练的方法不一样
第一他把他的视觉编码器skill上去了
就是它的世界编码器有更大的参数嗯
这里是6B的参数嗯
第二点呢就是他也是做呃类似clip的对比学习
但是clip那块做对比学习呢
是学完之后就把那个visl encoder拿来去呃
做多模态大模型
这个text encoder一般都是丢掉的
但是在那个intend v it训练的时候
它这个世界编码器
就直接和大圆模型的编码去做对齐嗯
把LLM换到之前的text encoder的位置
就是直接跟它对齐
在后面再做生成任务的时候
直接用这个LM
这样的话它的对齐效果会比嗯
跟一个即将要丢弃掉的
文本编码器去做在一起呃
效果可能会更好
它在预训练阶段就有一些呃天然的对齐了
在后面再做对齐
就会比前面那些呃要适配的多
对于它的1.2版本做实验发现呃
它的模型的倒数第四层特征最有用
那么就把后三层的砍掉
只要前45层
并且把分辨率从原来的224扩展到448
同时使用了更好的训练数据去做训练
在intend v l2中
对intend v i it也做了再次的升级
升级到1.5版本
引入了动态的分辨率和更高的更高质量的数据
实现动态分辨率的方法
类似larva next
在后面会详细介绍这一点里面的整体框架图中
观察细致的同学可能已经注意到了
有个模块叫pixel sher
为什么要引入这个模块呢
我们来算一笔账
就如一对
如对于一个分辨率为48×48的图像呃
v it的一个patch的大小是1414除一下
得到32321024的PH
也就是说一个图像会被打成1024的PH
然后这些patch得到1024个token
分辨率更大的图像切分后占用的token也会更多
要知道通常的LLM的上下文长度也就8K
32k
如果得到了token
直接送入LLM
那么一次对话处理不了多少图像
况且视觉模态有许多荣誉信息
这么多的token
不管对拓展多模态
上下文是对计算资源的节省都不利
所以我们要想办法压缩一张图像的token
在这个工作中
他们就采取的是pixel shuffer的技术
简单介绍一下什么是pixel shuffer技术
这个技术来自于超分那边
起初是做上采样用的
他就是把不同几个通道的特征拿出来
拼到一个通道上
具体而言看下面这张图嗯
刚开始的通道数是C乘以R方格通道呃
长宽是H跟W以R方个通道为一组
把特征都拿出来
把他们打散后拼到一组呃
就像这个示意图一样
刚开始是每个通道用不同的颜色去表示
然后把它们每一个拿出来
然后呃拼到一起
得到右边这个五颜六色的这个块
就相当于呃从特征去拿数字来拼到长宽上来
把长宽做上去呃
这样的话就可以把一个C乘以R方HW的图像
呃的特征转化为C长于C呃
H乘RW乘R这样一个呃数字特征了
这个N是bad size嗯
这里的R就是上采样因子
这个是类似于超参
可以设置呃来配置你想要上采样的程度呃
具体而言在INTEVL2中是怎么做的呢
呃他这里是把那个采样因子R设为了0.5嗯
这样就相当于是下采样
就把原来的4096×0.5乘0.5呃
这个通道数还有长宽为32的这个patch呃
把它打成4096
32×0.5
32×0.5啊
这样的一个特征了
那么现在他的那个拍摄的数量就是
16×16
只有256个头盔了
就大大减少了一张图片所需消耗的token
压缩的信息呃
有利于扩展多模态的上下文的同时
也有利于减少计算资源
他们采用了动态分辨率的处理方案
使得模型刻意处理各种分辨率的情况
首先预设了一些长宽比
考虑到计算资源最多设置12个tale
也就是说一张图像最多切成12块
所以在考虑M乘N
还有MN都小于等于12的约束的情况下
就最多有35种长宽比的配比
可以应对绝大多数的图像长宽比的情况
给定输入图像匹配与其最近的长宽比
把它reset过去
然后切成448448的tail
最后为了让模型感知全局信息
需要把原图reset到4484481块编码后
汇入LMINTEVL2
还做了多任务的输出处理
他们利用之前也是AI live的一个工作呃
visol y m v two用的技术初始化了一些任务
embedding和路由的token
当生成这些路由的token的时候呢
把任务的embedding和路由的embedding拼在一起
经过LM编码后
送给各个任务的解码器去完成视觉生成
检测分割和姿态估计等各种任务
讲完了结构设计
讲一下训练
他的训练方法
和之前的多模态大模型的训练方法
没有什么不同
也是分为两阶段
第一阶段呢是做预训练
这一阶段只训练MLP
使得其具备初步的视觉文本对齐效果
第二阶段做视觉指令微调
激活所有的模块
在视觉指令任务上做训练
提高模型的视觉质量
遵循能力
训练的效果很大程度上与数据相关
可以看到他用了各种各样的数据
这里就不展开讲
这一页给了两个INTEVL2
他们给的case
左边这个是做检测任务哦
给了一张闹市区的图片
让他检测出这其中的车辆
还有公交车之类的哦
可以看到这个模型可以呃找到所有的这些目标
并且给它标一个模框
然输入它的坐标
右边这个呃展示了他的连续图片的理解能力
就是给了一组照片
问他这一组照片一起讲了什么故事呃
在下面还给了这个照片的
就是顺序相反的一个结果哦
这边是穿衣服
那这边就是脱衣服了
可以看到INTEVL2的呃
回答的结果还是比较好的
可以看出他可以理解因果关系
还有事件的先后顺序
以上就是关于理论部分的全部内容了
下面该进入愉快的实战环节了
工欲善其事
必先利其器
先介绍一下我们部署微调用到的工具
第一个是我们部署要用到的工具l m deploy
这是一款由上海人工智能实验室推出的
高效部署工具
我们选用它有四点理由
第一是高效推理
可以看左下角这张图
纵轴呢是每秒的回复数量
也就代表我们推理的吞吐量嗯
这个蓝色的柱子呢代表VLLM的结果
这个橙色的柱子代表lm deploy的结果
VLM是另一款呃高效部署工具
可以看出在不同的模型架构和不同的规模上
lm deploy的吞吐量始终要高于VLM
第二点呢是高效量化
它可以让四位推理
性能比FP16高出2.4倍
也就是说让更小的位宽带来更切实的
更快的推理速度
可以看右下角这张图
橙色的是四位为宽
蓝色的是16位位宽
可以看出四位为宽的推理速度
比16位为宽的推理速度要高高上不少
第三点呢就是轻松部署分布式服务
可以简化多模型服务部署
最后一点呢是交互式推理模式
可以缓存对话历史
避免重复处理
这一页展示了l m deploy推理的代码安装
直接使用pip install inststyle
l m deploy即可
非常容易安装
然后这边是语言模型推理的pipeline
pipeline指的就是流水线
就是呃这个工作流程是怎么走的
是大概是这个意思
可以看出他的pipeline是非常简单的
你只需要import一下这个包
然后用你需要推理的模型去初始化一下
这个PIPLOG
接接着把你的输入以列表的形式传给pipeline
即可得到批量化输出
可以看出i m deploy的使用还是非常简单的
在这张图呢是部署多模态大模型
做多轮对话的一个小例子
呃
也是比较简单的
我们来看一下前两行呢就是import1些相关的库
下面呢就跟上面一样
也是对pipeline的一个初始化
只要传入模型的名字即可嗯
接下来呢需要把我们做多模态推理
需要的图片LOL进来呃
这块只要是读进来是PL格式的图像就行
不管用什么方式去捞的
然后设置一下生成生成参数
接着把图文对
以元组的形式和生成参数一起位给PIPLO
就可以得到一个对话的输出了
这个输出呢是个session
它可以存储单轮对话的输出和对话历史呃
比如现在没有对话历史可能看不出来呃
可以直接让他输出一下这个session
点response
点text
额就可以得到本轮的输出
如果你要进行下一轮对话
那么除了要传入当轮的输入之外呃
还可以传入对话历史
就是把这个session传进去之后
他就可以结合对话历史跟本轮的输入
去综合的进行一个输出呃
同样也可以用session点response输出单路的输出
如果你想要获取它的历史对话信息的话
可以用session点history来输出完整的对话历史介绍
完了lm deploy的特性和基本的使用方法
下面我带大家配置一下lm deploy的环境嗯
首先呢登录我们的普宇开发机平台
用你们自己的账号和密码登录一下
应该里面有书生实战营发送的算力点
然后我们可以点击创建开发机
在这里选择个人开发机即可
嗯开发地名称可以随便输入一个
比如intend vl
然后可以选择一下具体的镜像
这里呢我们可以选择COA12.2的这个镜像
点击使用
这就是我们镜像
接下来是资源配置
这里呢我们可以选择呃
0.5张100就足够我们实战应用了
剩下的不用管
然后点立即创建即可创建我们的开发机
在这里排队等待一会儿
开发机就会创建完成嗯
可以看到我们的开发机已经创建完成了
然后接下来我们使用vs code去连一下这个开发机
点这里的SSH链接会弹出一个登录命令和密码嗯
接下来我们打开vs code
我在VSCODE中
我们需要安装一个插件
嗯如果大家会使用vs code连
通过SSH的方法去连接服务器
那大家可以跳过这一步
就是安装一个这个呃ROMESSH之后呢
你左下角会出现一个这个就是open remote window
点击open a remote window
在这里显示一一堆选项
选择connect to host
然后选择configure SSH host
嗯选择第一个你的配置文件
打开SS配置文件之后
可以根据我们刚刚在开发机平台得到的信息
填写这个配置文件呃
它的格式要大概是这样
第一行host就是你要把这个主题叫做什么名字
这块可以根据个人喜好填写
咱可以叫INTEVL
第二个是那个主机名嗯
这里就需要填写你要连接服务器的端口号
或者是那个域名
这里呢
这里呢我们回去看一下那个SSH连接的命令
可以看到它的命令这块是SSH杠P
杠P后面这个数字是多口号
然后root root
这个是用户名
艾特后面这个东西是咱们的主页地址
我们把它复制一下
然后填写到刚刚这里
嗯接下来要填写端口号
端口号就是刚刚讲的是这串数字
现在pot这里这个这个是端口号
然后user就是咱刚刚看的root嗯
后面这俩是为了方便公钥连接的参数
这里就是按照我这这样设置就行
然后可以保存一下
保存完之后呢
我们这里的remote explorer
这里就有了我们的主机INTEVL嗯
可以点击这里打开它的文件夹
机器打开之后
这里我们选择LINUX操作系统嗯
如果你没有绑定公钥的话
会这里会弹出一个让你输入密码
把那个开发机平台这块的密码输入进去即可
如果想要不输入密码
可以点这里添加公钥
呃
然后在你呃自己的笔记本上创建一个SSH的公钥
放进去即可
这里我就不做演示了
想要更加方便连接的
大家可以上网查询一下
如何用windows生成这个SSH公钥
嗯进来之后就连接上了
我们就可以随便打开一个文件夹
比如我们进入这个root文件夹
可以打开一个命令行看一下
额这里就是我们的开发机的连接
下面我们进行一下这个环境的配置
首先呢我们需要创建一个虚拟环境
用conda create命令来创建
然后接下来要输入那个环境的名称呃
这个环境的名字可以根据自己的喜好去命名
这里就选用了lm deploy
然后指定一下Python版本嗯
再输入一个杠Y可以一会儿免得多打一个Y嗯
这样就可以创建一个新的环境
名为l m deploy
接下来等待它解析一下
环境配置完成之后
可以用conda create来激活呃
conda activity来激活它
然后呢我们就进入了i am deploy环境
激活i am deploy之后
我们需要用pip安装相关的包
呃
这里呢有些包对版本有要求
为了防止出错
建议你们去复制我的那个read me
呃就是在这里
推理环境配置这一块
直接复制这一句
Pure in style
I m deploy
还有图形化页面需要用到的GRADU
以及一些相关的依赖包
粘贴到命令行里
然后点击回车即可安装
接下来呢我们需要在GITHUB上拉取一段代码体验
在网页端部署intend vl呃
这个文档呢就是我们本次课程的read me
大家在呃TOTORRELL这个仓库里可以看到嗯
可以直接复制这个命令
粘贴到我们的vs code里
拉取下来之后
在这个INTELVL2TOTORIAL这个文件夹里哦
我们可以看到这个demo
就是我们启动网页应用的一个文件夹嗯
在这里有个参数
就是model pace是设置你要传入的模型的路径
在这个璞玉开发机平台呢
这个share文件夹下的那个new model的
下面就有我们今天要体验的INTEVL22B
如果你使用的是普语开发机
来完成这这堂课的实操
这个路径可以不用修改
否则的话需要修改到你自己的INTEVL2
下载的路径下
修改完成这个之后
我们在这里进入额当前这个文件夹
然后用Python
demo点PY启动这个网页应用
这里需要大家耐心等待一下
现在屏幕上输入这两个是我手误输入进去的
大家不用管
看到vs code弹出的提醒
就代表我们这个网页应用启动成功了呃
vs code自带端口转发功能
它可以把我们部署在服务器上的服务
转发到我们的本地
可以点击一下这个网页链接
即可开启我们的网页服务嗯
现在大家可以看到我们这个网页UI界面嗯
就大概是这样
然后这边开始聊天之后
在这里可以传入图像
这里可以进行一些简单的生成参数的设置
呃下面还有一些为了快速输入
我放了几个例子
这里是几个快捷输入文本的选项
下面是一些快捷输入图片的选项
由于今天我们实践的主题
是让LLM学会品鉴美食
所以我放的例子都是美食的图片
嗯这里图片传输也受限于网速
大家请耐心等一会儿
这里我们的图片加载完成了
可以看到我放了好多种不同菜系的图片
新疆菜
川菜
西北菜
前菜
素菜
粤菜呃
还有这是这些各种菜呃
我们可以看一下呃
INTEVL对这些食物的识别效果
嗯可以点击开始聊天
然后可以随便选一张图片
就比如说呃东北地区的锅包肉吧
可以问他呃
去哪个地方游玩时
应当品尝当地的特呃
你当品尝当地的特色美食
图片中的食物
输入进去后
它就会开始推理
呃他回答了说这张图片展示了一张美食
看起来是炸鸡排配蔬菜
如果你去泰国可以这样呃
可以看出他出现了幻觉
这张图片实际上是锅包肉
但它却把它识别成了炸鸡配蔬菜
泰国菜
所以我们后续需要对它进行微调来呃
强壮它对品鉴美食的性能
这里我们看一个比较成功的例子呃
点击start chat之后
在这里可以把图片拖上去
或者是点击让他去找一个文件夹
我们这里拖一张图片上去
比如说东北大学的雪景
输入图片之后
可以让他描述一下这张图片
呃输入之后好
INTEVR就看出了
这是一个一座城市在冬天的景象
然后有一个大型建筑
这个是东北大学的图书馆
可以看到有周围有人活动
可能有人活动嗯
旁边有好多摩天大厦
可以看出它识别这种呃
生活中的风景照还是比较呃细致
比较流畅的
也体现了这个模型性能的强大
当我们对话结束的时候
可以点呃clear context就可以进行下一轮对话
这次我们测试一个稍微抽象一点的图片
问他这张图片里有什么
呃他也可以看出是一个温馨的场面
里面有两个人物
这个成年人和孩子不太对
还是有一些幻觉的
这个二次元图片是视觉语言模型的大敌
但是基本上还是对的
就是头上戴着粉色的发带
然后在寒冷的天气里有冰和雪
另一个人戴着白色的帽子
白色的外套
刚刚我们演示了实战如何配置环境
以及我也应用了启动
相信大家已经上手把玩了一针
接下来我们继续回到PPT的内容
呃
我们再来看几个case
第一个呢是这个
我们上传了一张长长粉的图片
问他去哪个地方游玩时
应当品尝当地特色美食图片中的食物呃
但是INTEVL2把他认成了是饺子嗯
给他传上一张我们贵州的红酸汤啊
可以看到呃
银泰VL22B呃
不小心把它认成了韩国的食物
可以看出
INTEVL22B对食物的理解与识别
是远远不如我们人人类的
这是因为呃
可能他训练数据中缺乏包含美食的图片
所以他缺乏相关的知识怎么办
只有调
只有通过微调才能让他获得新的知识呃
通过微调
我们可以把一些美食相关的知识
灌输给英特VL2
让他学会相关的美食知识
这里介绍一下
还是由上海人工智能实验室开发的微调工具
x tour呃
我们选用的选用它的理由有
第一个呢是模型支持
它支持很多种LLM和VLM
VOM就是视觉语言模型
就是今天我们讲的动漫艾达模型嗯
第二点呢是数据格式
它可以支持任何格式的数据集
包括开源的格式
还有你自己定义的格式都可以
第三点呢是它支持多种训练算法嗯
他的训练算法包括QLAURA
Laura
全在微调之类的
可以满足你对微调的不同需求
还有一个就是高效性
高效性可以看右边这张图是一个生动的诠释
这个是这个纵轴代表训练效率呃
粉色的是这个是我们x2那的效果
蓝色的是我们另一个知名的开源微调
大模型的库number a factory的效果哦
可以看出
在8K上下文和32K上下文的情况下
我们的x2那性能始终要比la factory要强强嗯
在上下文到达128K
256K呃
甚至em的时候
我们使用level factory会炸显存
但是我们的x ta依然可以进行微调
这就体现了X从那的性能的强大呃
它支持几乎所有的GPU的微调
还有它性能优化的很好
就是右上角介绍的那样
它可以自动调度高性能算子
来提高训练的吞吐量
还有一点呢就是deep speed的兼容性
deep speed是一个加速训练的一个库
它兼容那个deep speed的多种zero优化技术
能够提高训练的效率
爱因用了都说好
有了训练的框架
有了待续的模型
我们要用什么数据去进行训练呢
这里我们选用的是这个呃福利QA呃
这个工作是今年中了今年的EMLP
它是用于细粒度
理解中国饮食文化的动态数据集
这是他的ARCAVE和汉语face子链接
大家有能力的可以去支持一下
去follow一下
下面给了几个示例
它这个数据集里有多图的VQA
有单图的VQA
还有文本的QA呃
比较符合我们的主题
它都是一些对中国美食的一些问答
Token is cheap show me the code
就是说讲那么多废话没用啊
不如直接给我看难吧
我们就上号x ta
下面我们进行一下x turner的环境配置
首先呢还是跟刚才一样
用conda create创建一个环境
这个name后面就可以输入你自己想要的环境名称
这里以S通那杠in5为例
然后指定一下Python版本
最后输入杠Y防止一会儿
然后重复确认
等待片刻便可完成新环境的创建
环境创建完成之后
可以用conda activate去激活环境
激活了环境之后
我们接下来进行一些相关于包的安装
先安装我们的x turner和一个相关的依赖包呃
这个包是XTNER所依赖的
但是它没有自动安装
所以需要我们手动安装一些PVE in style之后
就等待它安装完成即可
安装完X从那之后我们还需要安装一个deep speed
是训练加速用的嗯
如果不想安装这个包
在后续的运行系列代码的时候
可以把最后一个参数去掉也是可以的
配置完了X通用的环境之后
我们需要修改一下PYTH者的版本
因为呃pa是2.5跟S通
那不是很兼容
所以我们需要呃修改一下版本呃
这个命令到时候可以去我的那个read me里复制
然后我们还需要重置一下transformers的版本
在使用S通道微调的时候
我们会看到呃
我们需要有一个配置文件
配置文件是干什么用的呢
配置文件就是用来放置呃
微调时的训练设置的文件嗯
就是说相当于是x ta的任务手册
然后X通那照着你的指令去办事
一般配置文件里有挺多东西的
巴拉巴拉这么多呃
具体而言我们看到的参数可能是这样的
就是呃很多了一堆怎么办
不要怕
我来助你
下面呢我会解读一下每个参数大概是什么意思
这样我们就可以自行的选择参数的配置
这块参数虽然多
但是都是比较容易理解的
大家可以呃那样仙子慢慢听我讲
听完可能就呃会了呃
第一部分呢是模型与数据相关的配置
第一个Python呢就是传入我们呃
带微调的模型的路径嗯
一会儿实战会告诉大家
具体这些参数要配置什么
这里就简单介绍一下每个参数是什么意思
这个data root是数据的根路径
data pace就是具体的啊数据文件的路径
这个image floor就是放置图像的路径
prompt template就是LM那个呃
prompt模板的路径就类似于聊天模板呃
如果我们选用那个呃INTEVL2
由于它的基座是INTELM2
所以这里就选用INTELLMLM2的呃prom的模板
Max length
最大的头宽长度
这一页呢是一些训练相关的参数
第一个呢batch size就是我们的老朋友了
就是一次呃一步的训练会给他多少个数据
第二个呢就是累积的步数
它有一个特殊的技术
就是梯度累积呃
我们知道如BYSIZE设置过大
它会炸显存呃
所以说我们可以在呃过完这个白人赛之后
把梯度存下来
再过几个拜月赛的之后
再一起把累积的梯度呃去做优化
这个就是梯度累积技术
然后这个呃这个参数就是你累计梯度的步数嗯
它主要用来模拟较大的白日size
就是这里就相当于模拟了4×2
等于八的白日size
可以提高训练的稳定性
这个是data loader的nb workers
就是开几个子线程去进行数据的加载
max epose就是最大的训练轮数
就是说你的这个数据要过几遍
这个是优化器的类型呃
通常是那个SDD或者是阿达玛W嗯
我们大模型的训练通常使用的是阿达玛W呃
优化器这个优化器对学习率不敏感
可以呃更加友好的调参
下面这个呢LR就是学习率
Learning rate
它呃一定程度上影响了学习的速度
贝塔斯这个是阿达玛优化器特有的两个参数
贝塔一跟贝塔二呃
它跟那个动量的加权有关
Wait decade
这个是权重衰减
这个是用来正则化用的
防止训练的过拟合
Max noon
就是说我们梯度的noon最大是多少多大
就是用用于梯度裁剪
你设为一
它就会把超过一的梯度把它裁剪到一呃
Warm up ratio
就是说我们要预热的比例
通常来说我们在训练模型的时候
就会有一个预热阶段
就在这个预热阶段
它的学习率逐渐上升
上升完之后
然后再逐渐下降
这样的话就相当于给模型摸索一下
那个参数的范围这块是多少步一存
还有最多存多少文件
这一部分是跟模型相关的一些参数
freeze LLM是是否冻结
LNM就是我呃是否要训这个lm phrase
Visual encoder
就是是否要训练这个图像编码器
这个LLM
LAURA这个这块是配置LAURA微调相关的参数
LAURA就是低质自适应微调
大家感兴趣可以去看一下相关的论文
它是一种高效微调技术
呃
这个R呢就是那个低值矩阵的秩
决定了低值矩阵的维度
这个R越大
你训练的参数量越大
它占用的资源也就越多
laura alpha呢这个是放松因子
就是在低质矩阵做乘法完之后
还需要乘一个放缩因子
这个laura drop out就是drop out的概率
report就是我以多少的概率随机丢弃一些神经元
这样的话就可以防止神经网络的过拟合呃
这块呢是配置文件最下面的两个load from
就是呃如果我训练一半它断了的话
就可以把这个设为你的那个中间保存的check point
来继续训练呃
result就是是否要在你中断的部分继续训练
配置网络环境下面
我们进行x ta的实战
如果你使用的是pi开发机
在root路径下自带一个x ta
直接使用这个文件夹即可呃
但是我这里是呃
我这里已经有一些东西了
为了给你们演示原始的S通道文件夹下
如何操作嗯
我在这个路径下又克隆了一个嗯
到时候如果在你们自己的机器上
就可以直接CD root杠S通
那就是进入呃
进入这个艾斯turner这个文件夹下
进行开发即可
但我这里演示如何从一个呃
原始的X通道文件夹进行实践
所以我基本是这个路径
大家到时候注意一下哦
打开STM之后呢
我们看我们看这个S通NER下的S通
NER这个文件夹
这个CONFIG里面放着一些模型的配置文件
它有不同的以不同模型名字命名的
一些文件夹哦
可以看INTEVL
这下面放的就是呃
我们要用的模型的配置文件了
打开VR
这里面就是一些他写好的
一些现成的配置文件嗯
我们可以随便打开一个
比如说2B的fortune
嗯这个样式就是我们刚之前PPT介绍的
它配置文件的样式
为了方便大家快速实战
在我那个仓库里面
我也为大家准备了一个找配置文件
就是这个在这个呃
INTEVL2total下的XTNERCONFIG这个文件夹下呃
放置我们今天要用的配置文件
可以把它复制一下
粘贴到我们这个x corner下面的这个呃
in turn vl v2所在的这个文件夹下
也可以用那个嗯模糊命令进行操作
就是在命令行里把这个啊配置文件移动
移动过去
具体可以看read me里的操作
这里我就不做示范
另一种方式
嗯大概解读一下这个配置文件的参数
刚刚在PPT里已经解读过了
现在我们用实际的代码带大家快速过一遍
这些配置这些哦参数是什么意思
第一个Python呢就是我们模型的路径
这里我们填呃
还是shell路径下这个哦
自带的INTEVL22B即可
接下来呢是数据的一些配置
这个数据的根目录呢
呃我把咱这次要用到的数据集
放在shell文件夹下了
呃大家可以不用
然后麻烦自己去下载了
当然有能力的同学还是建议自己去哈
云菲子上下载一下
支持一下原作者数据的路径
就是我们这次要用到的这个文件
是跟PPT里讲的一致
是单独的VPA
然后这个图片的文件夹路径
这个我们跟数据跟目录放的放的是一样的
所以直接让它相等于相等即可
这个prompt temple template
就是intend l m to的聊天模板
最大的长度JD8192绰绰有余
下面的by size
这个需要根据显存设置
我们40G的显存呃
白色设为四就差不多了
再大可能会炸
这个是梯度累计的步数二即可
大家也可以调好参
自己玩一玩这个训练的EPOKE呃
为了让他达到出色的效果
就是在美食识别上有完全的性能
我这里选择十个1poke
这里有很大概率会过拟合
但是对于我们这个美食任务
过拟合完他的表现就好了
大家可以自行调整这个参数
去均衡通用性能跟美食性能之间的平衡
这个优化器选择二打码
就是默认的学习率选用三一负五
大家也可以自己调整一下
还有贝塔选择是0.9和0.9
九九开一个衰减
防止过拟合
这个开关其实意义不大
可以设为零
也可以设为0.05
因为都已经十个一空格了
保存部署呃
我这里是如果by size为四
梯度累计为二的话
我这个是64
不会于1poke
我就设置了64
大家可以根据自己的喜好去设置保存步数
这个total limited就是最多保存几个checkpoint
一就是无限个
下面这个是模型相关的嗯
配置
这块他冻结了LLM和视觉变换器
是因为要训练那个LAURA层
就不需要训练这俩呃
大家大家伙的本体
这个是LAURA的配置
而是那个低质矩阵的秩
这里大家也可以是一个可调超参数
大家可以动手调一下
这个是那个比例因子
这些是一些呃data loader的配置
这里就不用管用
默认的即可
这个是优化器的设置
嗯这里是它的学习率调度的配置
可以看出刚开始是线性学习率
就是从一个地方开始
他这里是从零开始
然后以这个速率慢慢增加这个学习率
当它长到一定程度之后
然后进行那个cos衰减
就是上去
然后根据cos慢慢去衰减
这个也是比较常用的一个学习率的调动方式
刚开始慢慢增长
是让参数慢慢探索这个参数空间
加入这样一个预热过程
会让模型的训练更加稳定
这些是运行时参数
这些都不用管
嗯最下面呢这个参数是load from
就你可以从一个checkpoint那里把那个模型LOD进来
继续训呃
如果你这里设置一个路径的话
这result resume要设为true
然后让它支持断点重新
这就是配置文件的大概解读之后
我们就可以利用命令去运行这个呃微调
呃看起来参数多
实际上每一个都不是特别复杂
相信听过我的讲解之后
你对这些参数有了一个大概的理解
接下来呢我们就可以呃启动这个模型训练了
我们直接调用x turner和train这个命令
再传入你的配置文件的路径就可以启动训练了
后面这个deep seed deep speed
这个参数是传入你要选择deep speed的哪个引擎
如果你不用deep speed训练
可以把这个参数忽略
只要x turner train加你个配置文件即可
下面呢我们来正式进行微调
它的命令是x2
那train
然后带上配置文件的路径
呃这里呢就是我们刚刚复制过来的这个呃
就是这个
负的这个配置文件
你们可以复制一下它的路径
然后放在这里嗯
如果你想要加速微调的话
可以传一个deep speed
这块忘了
可以去看一下我的那个read me里是怎么写的
就是传一个deep set zero
二用这个来进行加速训练
按下回车键即可启动微调
呃大家看到这里报了训练日志
即为训练成功启动
他会一步一步的报loss嗯
如果大家想早一点看到现在日志的话
可以调整一下呃
一个超参数可以修改一下他报告的部署
就不用等那么久
大家如果着急
可以改一下这个logo里面的这个呃
interval这个参数
把这个改成几
就是它迭代几步
它训练几步之后会报告一次呃
训练信息在训练完成之后
在我们的XTM文件夹下的work directions
还有这个跟配置文件相同名字的这个文件夹下
会有我们训练的checkpoint
就是ATOR几
就是代表第几步保存的checkpoint嗯
后面这个数字越大就是越靠后的checkpoint
这里我们可以选用最老的checkpoint
把它进行格式转化
一会方便我们进行测试
呃还是打开我们的read me呃
在这里给出了转换文件所用到的命令
可以复制一下
嗯我们还是需要进到我们的x corner
这个文件夹下运行这个命令
呃前两个是转换这个模型所用到的PY文件
这里不需要修改
然后这个是第一个
这个是呃
你刚刚选择的那个最大的那个拆矿的路径
这里我选用了相对路径
也可以填写绝对路径
后面这个呢是你想把转换后的模型
放在哪个路径下
具体而言
这个第一个你想得到的这个
你要转换的这个模型的路径
可以通过vs code的copy pace来获得
就是你看中这个文件
点个右键
然后copy path
然后再粘贴就可以得到这个路径
因为我这里是配好的
如果你没有修改成参数的话
你也就不需要修改这个check point的路径
就跟我设置一样即可
如果你调整了超参数
它最后训练的步数可能发生变化
则需要根据刚刚教的把那个路径改一下
在这里传入应合适的路径
按回车运行之后
我们就能在这个路径下得到
方便测试的格式的模型了
嗯没有报错即为运行成功
嗯训练完训练完模型之后
我们可以修改刚刚演示的这个model path
这一块就是那个demo点PY里的model pace
就可以与你自己训练的模型进行玩耍了
在模型转换成方便推理的格式之后
我们就可以打开之前我们部署用的这个demo
点PY文件
然后把这个模型路径修改为
我们刚刚转换过后的路径
把他这个路径复制一下
然后在这里替换上
保存一下文件
然后再次进入
我们刚刚嗯推理所用的这个路径下
呃这里别忘了切换环境
切换回我们之前推理时用的环境
我之前演示的是lm deploy
请大家根据自己当时的命名切换环境
接下来呃运行Python demo
点PY即可启动网页部署服务
去体验新的模型
启动网页应用之后
同样是可以点击这个网址进行访问
这里就打开我们的网网页应用
同样需要稍微等一下下面图片的加载
我们这里再次测试一下他对食物的认知
点击start chat
同样选择刚刚他错误识别成炸鸡排的锅包肉
可以问一下这是什么食物
嗯可以看到微调后的INTELVL2能够正确的认知
这个才是锅包肉
看来我们达成了我们想要的效果
这一页展示了微调的效果
在微调之前
某些会把肠粉认成饺子
微调之后它就可以正常识别这是肠粉
它会把这个锅包肉认成叉烧包
或者是我们之前演示的那个呃炸鸡排
在微调之后就可以正确识别出
这个图片是锅包肉
恭喜你完成了本次课程
你已经学会了多模态大模型的基本知识
学会了多么爱大模型的部署微调实战
恭喜勤劳聪明努力的你完成了本课程
太强了
你已经成为了多么high
大模型方面的专家
哦这是本次录制课程用到的一些参考文献
大家感兴趣的话可以自行阅读一下
谢谢大家
我们有缘
下次再见
拜拜