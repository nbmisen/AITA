大家好
欢迎来到初生葡语地质战训练营啊
我是今天的呃
主讲北辰
然后我们这一期主讲的内容是
使用我们的书生葡语英特LM呃
以及lama index框架去实践一个RG的项目
然后这个课程是由助澜yo
以及我我们去共同呃合作的
额我们今天主要的内容会分成三个部分
第一个部分是我们会特别简单的回顾一下INTEL
就是书中葡语的一个呃历程
然后我们会介绍一下RG
帮助大家理解RG到底是什么原理啊
怎么应用
然后它的框架呀
评估测试等等一些呃
使用的时候
入门的一些基础的概念和需要的呃知识
呃
最后呢就是我们的重点
就是实践一个RG的项目
使用到的就是lama index框架呃
应用的就是我们最新的INTELM的书成补语
最新的2.5模型
那我们先简单的回顾一下
我们书生葡语的这个呃发展历程吧
呃书生葡语大模型INTELM呃
呃模型是2023年7月份首发的7B呃
它是国内比较呃率先呃开放的一个商用
可以完全免费商用的一个呃大模型
然后呃他的优点就是嗯
当时初中葡语发布的时候
不仅仅是一个单一的模型和API的
同时
也发布了整个链全链条的一个呃开源工具
呃
估计大家在完成这个课程之后就会了解到
从这个呃模型的训练
一直到模型的应用量化到测试都是全套的工具
然后在去年的9月份的时候
INTELM的第一版的20B模型也同样开放了
同时我们的这个工具链也做了全面的升级
在今年的1月份的时候
英特尔i m do to模型嗯
嗯就是二代的升级版嗯
进行了开源呃
在当时发布的时候
它的性能是超过所有同期的呃
开源模型的
那在7月份的时候嗯
我们书中葡语又开放了2.5的模型啊
也是我们这个第三期实战营所用的
一个主要的模型
那这个就是一个对初中葡语呃
开源模型的简单回顾
然后我们进入到今天比较重点的部分
就是介绍一下IG
我们首先对lg这个技术做一个基本的概念的理
解和呃定义
那RG的全称是ritual augmented generation呃
是一种结合了检索和生成的技术哦
中文呢叫那个搜索增强生成技术呃
他的目的是通过利用外部知识库
来增强我们已有的这种大模型呃的能力
那通过检索与用户输入相关的信息片段
结合这些呃信息
然后让大模型生成更准确
更丰富的回答
那大模型其实呃
我们主要在想要去解决使用这种技术
主要是想要去解决大模型出现的一些呃
共有的比和比较
就实用当中比较常见的问题
一个是生成幻觉
就是他不知道的内容
然后大模型有的时候会硬生成一些片段
就是估计大家也在网上看到很多这种呃效果
然后第二个就是关于过时的知识的问题
就是因为我们的模型训练都是一次性的
不是实时的呃
所以当我想要问大模型一些最新的消息的时候
及时的信息的时候
那他其实是没有办法给出准确的答案的
那第三一个会解决大模型的问题
就是缺乏透明和可追溯的推理过程
就是我们一般大模型
它给出的答案可能是正确的
但是我们呃不太了解他是怎么样呃
给到这个过程
所以我们可以通过IG这种技术来够
帮助我们去理解
他到底是怎么样给出这样答案的
所以呃整体来讲就是IG技术是解决呃
大模型在处理知识密集型任务时
可能遇到的这些种种挑战
然后可以为我们整个的应用提供更准确的回答
那降低我们的推理成本
实现呃完整的外部记忆啊
我们其实现在已经有很多的
这个关于RG的应用了
包括问答系统
文本生成信息检索
然后如果是多模态的话
它还可以有图片描述
这些
这些应用其实都是可以应用到我们RG技术的
好
那我们现在开始简单说一下
我们RG的基本的一个工作原理
就是lg他这种技术不是单纯的某一个训练
或者什么
它更多的像是一种概念型的框架呃
这样的一种方式
来帮助我们这个提高大模型的啊能力
那通常来讲我们会有一些外部的一些知识源
那这些知识可能包括最原始的像文档啊
网页呀
就是我想让模型去找到的信息
他有可能是及时的消息
有可能是很专业的
很少的一个信息
那首先我们一般会把这些知识库进行一个索引
这是整个RG工作流程当中很重要的一步
通过索引编码成
把这些所有的外部知识编码成向量
然后存储在我们的一个向量数据库当中
就叫vector dB
那我们这种vector database vector dB呃
有了之后呢
呃我们的使用方式就是正常使用方使用的时候
我们的用户会直接去问一个问题
给到这个大模型
然后大模型生成之后返回来
那中间会出现我们刚才上述举的种种例子
像幻觉啊
过时信息
那现在有了RG之后呢
我们的工作流程就变成了模型
会先去在这个vector database当中去进行一个检索
那这个检索这一个过这个动作
接收到用户的问题后
它会将问题也编码成向量
然后在数据库当中找到最匹配的
这种相关的一些文档块啊
或者是知识块
然后把这些知识块输入到大模型
然后大模型再根据我们用户的输入内容
问题和这些知识块儿
再根据提示词共同生成一个答案
那这个就大家可以这样的话
就比较直观的能理解
为什么说我们这种RG可以解决到这种呃
密集型知识
然后歧视信息等等这样的问题
那这个就是一个基础的一个流程
基本上任何的呃
RG的应用都遵循这样的一个框架
然后我们现在来讲一下这个向量数据库
vector database的问题
因为其实这个vector base也算我们lg技术当中
一个重中之重
因为如果没有他
你也没有地方去检索你有没有排序
然后你的相关的外部文档也没有
不存在这些东西
所以呃vector d呃
database这个是我们的一个很重要的一个范畴
那首先是这vector database呃
承担了我们整体的一个数据存储的任务
它将我们的文本和其他数据
通过呃相关的预训练模型啊
或者其他的一些方法转化为固定长度的向量
表
示
这些向量呢就需要能够捕捉到文本的语义信息
然后在这个向量库当中进行相似性检索呃
就是通常我们会使用向量数据库中
找出和我们问题当中最相关的向量过程
那通常情况下我们会通过计算cos的这种距离
或者是dot的product
这种像点乘的距离来进行相似度的判断
当然也有其他的这种相似度判断的方法
那检索的结果会根据我们的相似度得分
会进行排序
然后把我们刚才提到的top k去给到嗯
我们的生成模型
那这个向量数据库当中
比较重要的是一个向量表示的方法
它决定了我们这种呃我们的数据存储的质量
以及我们检索得到的数据质量
然后我们表示方法可以有
我们使用其他的这种嵌入式的大模型
语言模型呃
或者是使用这种句子嵌入
段落嵌入等等等等的一些技术
那这一块涉及到一个呃比较实用的点
大家可以了解一下
就是我们在使用的时候
因为语言的不同
所以可能更多的会选择贴近我们这个语料源的
这种语言的嵌入模型会比较好
现在通常使用的都是基于大模型的
这种嵌入模型
也有很多的开源应用
嗯然后我们简单的回顾一下IG的发展进程
那RG最早的概念其实是由是由meta
就是FACEBOOK的路易斯
这些人在2020年推出的论文当中提的
这个论文的名字就叫ritual augmented generation
For knowledge
Intensive l l p tasks
呃
所以它其实是一个像今年应该算4年了
它其实是一个比较新的概念
他最早的时候其实一个naive的RG系统
就是完全等于说
就是遵循我们刚才提到的那个流程
就是文档
然后进行一个索引
然后检索
然后提示词和我们的大模型去进行生成
这个通常来讲
就是应用于我们的问答系统和信息检索
现在更多的进一步的时候呢
为了我们更好的一个检索结果和需求
我们有了这种需要的摘要生成呃
内容推荐等等的应用
会选择这种更高阶一点的这种RG
有的west的RG
那它的一个特点就是哦
除了在我们呃原有的这种过程当中
除了说进行一次搜索之外
它其实多加了两次
一次是在我们检索之前进行了一个前检索
我们对于这个用户的问题
其实我们就进行了一次呃检索
那同时呢我们也有后检索
就是当我们检索之后的内容
我们可能会使用排序呀
然后进行这种内容提取呀
或者是融合的方式再进行一个优化
那以此来达到这种呃更好的一个效果
然后形成摘要
形成内容推荐
更准确的推荐匹配吧
然后再通过提示词进行这个呃大模型的生成
那最近的研究方向就是这种模块化的RG
其实是一个主流方式
它可以应对我们这种多模态任务的需求
和这种连续对话系统的需求
那这个图其实就比较好的展示了
这三种不同的区别
那看到这种多模态就大家就可以看到
那其实它是通过这种模块化的行为
把不同的整个的工程的内容
然后分部分放进去
根据我们实际的操作
然后就进行提取了
那RG其实有很多的这种优化方式
它并不是说我建立了这流程
他就能很好的查询
它
还需要我们对每一个模块的
进行一个细部的优化
然后每一个模块细部优化有很多的技术
那这里边就是说呃
按照我们整体的一个大的流程框架
简单的找了一个主流的方式吧
那第一个就是进行索引度优化
我们可以选用这种细细粒度的分割
那这个就一定会考虑我们这种数据源
存储的时候
大小也可以用原数据存储等等
那还有查询时候的优化
就是我们要查询拓展呐进行转化等等
那还有嵌入过程的话
就刚才也提到了选择合适的这种嵌入模型
然后也可以说结合系数和密集的任务
然后或者说进行多任务的这种潜入嗯
然后这种上下文的管理
就说我像刚才说的这种后那个post retrieval
后检索的时候
就是我已经拿到了这个呃
检索的内容
说要进行再进行一个重排
或者进行一个上下文的压缩选择压缩
然后有一个更高效率的额选择
当然这种微调就在检索过程当中生成过程
双重的这些微调其实也是一个很重要的过程
就是后面我们会想到微调和RG
它并不是一个独立的部分
在一个完整的应用当中
其实是应该相辅相成的
那然后是一些针对专门针对于检索的一些优化
包括迭代检索
就是进行呃反复的重复进行生成
一直到查到的这些为止
然后也有递归检索
然后包括我们也可能比较熟悉这种列式推理
这个概念
还有自适应检索
就大家有兴趣的话
可以看一下这个相关的论文
或根据关键词搜一下
方法是有很多的
那刚才也提到了
其实呃在我们正常的应用
无论是学习还是现实的工作场景当中
RG的方法和微调的方法
其实都是一个有类似于博弈的状态
那先简单的说一下它们之间的区别和不同的
RG其实是一种非参数的记忆
就是我不需要对模型进行本身的大
模型进行任何的公改
我是利用外部的知识库
提供这种实时更新的信息的
然后它能够处理这种高度密集
高度实时化的信息
然后腿提供这个基于事实的答案
那当然也可以通过这种检索的增项
提供更加多样的内容
像我们最喜常用的可能就是一些呃工具
它会去搜寻这种检索网络上的内容
然后根据网络内容再提供一个哦
回答它适用的场景也也也是很明确的
就它适用是需要最新信息和实时数据的任务
而开放域的问答
实时新闻摘要等等
优势就是它对动态知识的更新非常好
然后他也特别适合处理长尾知识问题
就是呃有些时候我的这个
我们的这些答案可能只有一份儿
那如果说我们只拿这一份的数据去进行
数据的话
就会出现这种数据的这种BISS很严重的问题
这种长尾的问题可能在微调上不会很好处理
那这时候IG是一个很好的补充
那他的局限也是非常明显
就是他非常依赖于外部知识库的
质量和覆盖范围
就是他还是遵循了我们整个的这种呃
大模型的这种garbage in garbage RT
一个原则就是他的回答呃
答案基于我们的外部知识库
如果你这个外部知识库呃
有一些瑕疵
或者是就是有一些是错的
那他回答是错的
但是他还有一个隐藏的问题
就是lg的功能
其实呃从根本上来讲
它还是基于我们大模型的能力
就像是我们说这种呃70B的数据的呃大模型
那理论上来讲它的性能一定是要好于7B的
那可能更要好于我们这种1.8啊
等等的这些这个小一点的模型
所以呃它真正的检索搜索来效果是呃
在现有的大模型的基础上做一个提升
那这种提升其实还是不能打破
我们整个大模型的这种scaling scale
loud这个范畴的
那这个是我们的RG的一个基本的一个
针对模型应用的一个优缺点和场景
那fran tune呢是可能大家都比较熟悉了
就是呃它是一种参数记忆
就是我要重新训练我的模型
就是我可以使用部分呐或者是一些技术手段
但我还是要重新训练的过程
那通过在特定任务数据上训练
这个让模型更好地适应这个任务啊
这个的前置条件就是它需要大量的标注数据
然后呢有一个风险
就是微调后的模型可能会过拟合
因为呃相较于它在最基础的模型训练的时候
我们的翻跳的数据可能会很小
也有刚才提到的
就是它有可能会出现这种长数据长尾的问题
那适用的场景FUTA就是适用于数据可用
需要模型高度专业化的任务嗯
像特定领域的文本分类
情感分析文本生成
就典型的就是类似于我们这种你像律师的呃
呃法律文件的一个处理
一个分析
还有包括我们医疗数据的一个处理分析
就是他对于实时数据要求没有那么高
但同时他对专业度要求很高的情况下
可能会选用fighting的方式更好
那他的优势就是很典型
就是针对特定任务的优化
就特定任务
直接这种点对点打击方式的一种优化方式
它的效果会很好
那他的局限第一个局限就刚才的反复提
就是他对于数据的大量标注
然后第二个问题就是
他没有办法有这种很好的实时性
所以呃看起来他们两个是相对的是比较
但其实更多的时候呃
整个大模型的优化其实是一个相辅相成的过程
它的核心是根据我们是否呃
有高额外部知识的需求
或者是有模型适配的任务的需求
来决定我们到底采用哪种方式或者混合方式
那这个表格就很好的展现了
我们就是现在比较通用的流行的啊
模型优化方式的一个呃
就是一个维度图吧
像这边的横轴就是它的模型适配度
纵轴就是外部知识的需求度
那FTL呢肯定是对于这种模型
适配度是很高的哦
但是呢他对于这种实时性就很差
那相对来讲
我们的这个呃哈提示工程
其实是属于一种比较初级的方式
它就是实时性也一般
然后呃模型适配其实也一般
那RG呢相对它和FANTINE
是一个看起来对立的场景
就是他对于实时性的提升是很好的
但是他对于整个的模型的任务适配啊
可能没有FTING那么强
但呃最终的话其实整体来说
我们是通过融合各种技术手段和方式
我既有PHANTON
也有prompt这个engineering
用RG整体方式
我们可以达到一个
既保证外部数据需求的高需求
也能满足任务需求的这样一种方式
具体还是要看我们的呃
任务的要求和我们的这种场使用场景吧
然后我们快速的说一下
这种评估框架和基础测试吧
因为呃我们可能很多的呃朋友还是有这种呃
学术要求
或者是自己的一个呃指标要求的
那整体lg这一块它是分两块来评测的
一块是检索的质量
一块是生成的
就是我检索到的内容
是否和我呃这种想要的东西是否是匹配的
匹配的度有多少
另一个是我生成内容的质量如何
然后他也有各种的评估维度
包括它的噪声啊
然后这个负面拒绝啊
信息整合呀
这种的反面的一个鲁棒性啊等等
包括还有一些诚信
还有的一些嗯特殊的是一种安全性的一个东西
就是呃维度很多
然后我们的经典评估指标
其实大家都很熟悉的
准确率
召回率啊
Iphone score
然后blue score
然后ROG这些都是很基础的
就是翻译检索文本生成
然后这这边是检索的一个呃准确率
这边生成的准确率
那针对RG的整体的一个技术的话
会有一些评测框架
像基准测试会有rgb record crude
那评测工具有regards eris
truly as等等等等
那呃这这里边有一篇比较好的综述
大家可以去看一下
我觉得对于这个初学呃
lg了解的这一块的内容的话
有一个很好的全面的理解
那最后我们来根据他这个刚才提到那篇论文的
这个总结图
诶
我们也来总结一下整个的啊RG
这就是刚才讲到的
他是呃应用可以这些范围
然后这个是它的发展模型
然后这是刚才提到的一些方式
可以说进行一些技术提高
然后这是一个技术手段
那比较重要的其实是这两块
就是一个是它的技术站
那其实RG是有很多的技术框架
可以帮助大家快速的去实现这个RG应用的
一个呢就是我们的long unchain
那另一个是拉玛index
这两个都是我们大家很熟的呃
比较流行的那也包括那个FLUIZ啊
Ai auto jin
那其实之前我们第一期的课程
用的就是long chain的框架嗯
大家有兴趣都可以去学习一下嗯
然后包括国内也有很多的框架
还有其他的国内外别的框架
然后哦
inter l m还有自己内部的一个孵化的一个框架
叫茴香豆
其实上回有讲
这回会放到高阶课程里
那呃关于RG这边其实主要的挑战呃
目前其实和大模型基本上是保持一致的
一个就是在长文本序列当中
因为刚才也简单的提到了RG的性能好坏
其实还是基于这个大模型本身的性能的
所以说嗯我们在RG给它输入的一些
相关内容当中
如果过于长
那模型还是没有办法处理的
然后也有这些问题
就是它的鲁棒性啊
包括这种呃这种产品的及时性啊
scaling law啊这些等等问题
然后多模态的话
其实现在发展的也越来越好了啊
这个刚才也都提到了是一些评估的问题啊
有需要的可以去自己看一下
好的那RG的概念我们就介绍完
下面我们进入到我们的实际操作部分
先简单的去呃说一下
我们这回会用到的一个框架是lama index
那la index是什么呢
它其实就是一个开源的索引和搜索库啊
他不是一个专门为RG设计的工作
它是一个针对大模型的一个整体的一个呃工具
然后他可以呃很好的去帮我们建立这种啊
我们的这种知识库
然后它也有一些很方便的工具链
帮我们去进行这种搜索
就是我们IG所需要各种啊
它能够提供高效可扩展的文本索引和检索功能
就是这个大家可以看到它
我可以进行这种数据的输入
嵌入式生成
然后包括大模型的连接
包括这种呃向量化数据的链接
然后评估等等
这些他都能做
然后它的应用呢就是这种问答知识库
然后建立这种结构型的提取
然后对话语义是搜索
然后这种agent这种就机身智能啊
其实都是可以应用的
那我们主要使用的呢是我们这种la index
在和他在RG方向呃应用
那其实这个图刚才和那个是基本是一样的
就是呃la index可以可以去呃把呃数据库
然后文本甚至API的内容进行一个索引
然后后面就和刚才讲的一样
就用户发一个问题
然后在索引里进行检索
检索到的内容和我们的这个问答的这个promote
包括我们的数据
然后一起给到大模型
大模型生成返回给用户就是这样一个流程
那leon index最大的特点呢
其实是在他的数据索引呃
检索和它的知识嗯模型上面
那LAMAINDEX非常好的一点
就是它对于大规模数据有着非常好的处理能力
它可以说嗯对于我们这种大规模数据进行索引
然后它也支持多种的文那个文件
文件的这种数据源
所以这一点是我们在使用当中会去啊
倾向于它的一个很重要的因素就是它很方便
然后呃数据规模上如果提高数据规模
它的支持性又很强
那第二就是它能够提供高效的检索机制
这一块非常非常的重要
就是我们可能不需要自己去
过于过多的去调整什么
直接去进行一种它已有的检索机制的选择
就可以帮我们找到相关信息
那同时呢这个la index虽然叫拉马index
但是他现在呢已经支持除拉马以外
包括GPT
也包括很多的谷歌也好啊
那个android也好啊
他们的这种API
也包括像我们的那个书生葡语系列这些大模型
它都是已经兼容支持的
基本上我在他的官网上可以找到
我们常见的所有的这些大模型
它都都能够支持
然后在la index在RG上的应用
其实是一个嗯特别方便的东西
就是它已经提供了整个IG在一般应用全过程
它把它进行了模块化
然后也拓展也非常的方便
那根据la index它的架构
整个的RG流程其实分这五个步骤
包括这种数据的加载
然后数据的索引啊
storing数据存储
数据查询效果评估
它是包括这五个模块
那数据加载方面呢啊
这个la index支持
基本上支持我们需要的所有数据源
那他有一个la hob
这个hub呢提供了很多很多
就是这种现成的这种它叫连接器
就是connect us horror
或者我们叫reader
就可以直接调用这个reader帮我们去读取
例如PDF啊等等这些工具就是很方便呃
那建立索引的过程呢
它也可以有很多选项
就是我们选择如何去嵌入我们的整个模型
然后也可以选择其他的这种策略啊什么的
都很方便的
然后存储这一块
就是我们在进行索引之后
LAMAINDEX都会直接帮助我们去进行
这个数据存储的工作
然后我们可以避免重复索引的过程呢
这一块是提高很大的效率和响应速度
那查询方面
拉面应盖斯更是提供了很多现成的查询策略
那包括子查询
多步骤查询
混合查询
那这些方法吧就是我们可以直接调用
然后也不用自己去写
就是在真实的现实场景当中
其实我们更多是看哪个方法
更适合我们的项目需求
而不用自己再去做测试什么
然后评估也是
就是我们直接调用它的评估方法呀
呃你像我们就可以它的准确性
鲁棒性速度等等
那直接去进行一个比较
所以就整体来说
这个它是提供了一个完整的工具包
我们只需要去进行一些选项就可以
其实自己搭建的步骤很少很少
那到此
我们关于理论的一些介绍和一些框架的介绍
就呃就到这为止了
然后我们进入下一部分
我们会进入我们的实践内容
那实践的部分话
就是我们会用那个书生葡语最新的2.5
和我们的la index
去直接搭建一个我们的这个新的一个模型嗯
搭建一个RG的系统
那这个系统的最直观的体现就是呃
我们会去问一个这个额
我们现有的这个拉额数声谱与模型一个问题
这个问题是他训练的时候数据当中不知道的
所以像我们在举例当中
你看我们就采用一个例子
我们就会去问他x tuna是什么
因为x tuner并呃的相关数据
并未包含在我们整体这个1.8
B模型的训练过程当中
所以啊这个模型回答就答非所问
就是他会认为ta是一款用于播放音乐的软件
但是在我们搭建了RG系统之后啊
我们这个系统就可以很好的就是同一个呃模型
就可以用RG的系统很好的再去回答这个问题
他就知道我们大家听呢就是一个高效
灵活全能的轻量化大模型微调工具
那么这个就是我们呃整体要做的一个
相关的实践内容
那我们现在就准备开始实践吧
嗯大家好
我们现在就进入到了我们整个的呃
RG的实战部分
选取的就是我们刚刚在呃内容部分提到的la index
和我们的英特尔i m two
说成普语二的一个呃文档
然后我们先看一下我们这个整个实战部分
包含哪些内容
一个是前置知识
第二是环境模型准备
然后就是我们的lama index的这个呃安装框架
安装之后就是我们的lamy index的实践
那我们前置知识部分的话
其实和之前的那个主要的理论讲解很相似
那这里简单再介绍一下
为了防止有些同学呃没有看到那个理论部分
那前置知识呢
其实呃主要是我们要知道RG是什么
就是检索增强生成retural augmented的generation呃
正常我们大家请大家在实际工作
或者是项目当中会遇到一个情况
就是我大模型需要生成的内容
它需要依赖一些最新的内容
或者是他训练的时候没有涵盖的数据内容
那这个时候我们一般有两种方式
一种呢是这种更新模型参数的方式
就是我们把这种外部的数据给它
即变成我们训练用数据采用翻T的方式啊
重新训练模型
那我们的第二种方式呢就是呃非权重方式
就是我们把这个外部的数据作为一种上下文啊
或者是这个东西给他引入大模型
生成的内容当中
但同时我们又不改变参数
就是我们跳过了这个训练的过程
那它的优点就是说一个是我们可以实时的去呃
处理一些实时信息
就是我随时从互联网上或者最新的消息检索
然后呢我也不需要呃重新训练
然后耗费很大的算力这样一个事情
那一般来讲这种非模型训练
你像我们常见的方式
包括了我们的这种呃提示工程
我们通过prompt的信息
给到大模型和大模型产生
但是prompt的问题是呃
prompt本身的这种生成
可能需要我们自己去进行处理
那我们可以简单的理解一下
就是我们把RG当做一种自动的去生成
这种prompt的一种技术
那呃整体的流程工作原理
就是我们将我们这种外部知识源进行编码
通过索引的这个流程编码成一个向量数据库
当我一个用户使用提出一个问题
无论是对话也好
还是想让他进行推荐也好
那我们会先把这个问题通过检索模块
在这个向量库数据库当中进行检索
找到它最高匹配的这种呃这个数据块之后
就top k chunks
之后
我们把这个检索到的内容和我们的问题
作为我们的这个prompt提示词
输入到我们的生成模块当中
那这个时候大模型利用这些呃
原始问题和检索文档
包括我们的一些额外的提示
然后最终生成我们的答案
那这个整个的流程就帮助我们解决了
我们说这种如何去实现不进行参数改变
不进行训练
然后实现外部数据库的一个建设
它这个下面这个图表也是我们的一个效果对比
然后我们采用的是我们呃
这次用的是我们的书中葡语二
然后1.8B的一个模型
那这个模型其实呃训练的时候
数据库当中没有收录到我们XTA相关的内容
所以当我们使用原始的这个模型去进行提问
说eta是什么的时候
他回答这是一款呃播放音乐的软件
那当我们使用了RG之后
它就会有拥有了这个新的知识啊
注意这里我们并没有训练
我们只是把这个作为一个数据库
新的数据库
然后通过lg这个框架搭进去之后
他的答案就是哦
这回就就能精准的识别
execution是一个高效灵活
全能轻量化的微调数据库
那下面就进入到我们的实操部分
然后我们一步一步跟着教程来就可以了
嘿嘿好的好
我这边同步开一个这种机器
然后进行一个操作
大家如果在自己有运作上有问题的时候
可以跟着我一起来做
首先按照他的说
我们要先建一个特studio开发机嗯
在这里我们选择一个80%
扩大11.17的镜像嗯
选择个人开发就叫
拉马index r a g好了
选择镜像11.7
这里选择使用
嗯这块可能需要等待一下哦
我们到时候剪掉吧
这块然后选立即创建就好了
然后这一块的话
我们可以选择直接进入开发期就进入了
然后先看一下
我们先找到我们的这个一个序列号
因为我们一会儿要使用一个哦透传的功能
这个应该在前置的任务当中已经有了
然后我先把这个透传建立起来
好了嗯
这个就透传的这个任务就结束了
然后我会选择直接在开发机上去进行操作
我没有进行远程的SH连接
大家习惯的话可以用vs code啊去进行连接
其实很方便的
在整个作业过程当中
哦然后我们进入到根目录下开始吧
从开发基建完进来
然后环境建设
先建一个康达的环境
嗯选择确认
老师这个是根据大家的网速呃
不是根据大家
就是根据我们实时服务器的这边的一个流量
使用情况
会有一定呃时间的变化吧
就大家不要着急
如果特别慢的话
就是换一个不太拥挤的时间
然后这边就选择已经安装完了
现在把所有的这些需要的包给它安上
其实我们这里边就安了一个python3点十的环境
后面还要我们去手动去安装
其他的一些相应的一些呃
组建package之类的
好然后其实他这也提醒了
其实我们安装完我们再检查一下吧
我们这个包是不是安装完了找一下
我们叫我这里比较多
La index
OK这个包已经有了
那我们就激活一下就好了
就是从现在起大家要注意一个事情
就是我们之后所有的操作都会进入
这个在这个lamy index这个环境下
那他的你检查是不是就是你在命令行当中
也可以看到你前面的这块是不是拉满index嗯
就是你看正常我们启动一个页面的时候
它都是贝斯
然后如果我们去做一个
他就是la index
就是我们要确定我们所有的命令行
都都是在la index才可进行下面的操作
回到这边
然后这时候我们要安装我们的一些呃相关的呃
应用的包了
我们先安装PYTORCH
我放到这里
我这里因为之前其实安装过这个环境相关的包
是不需要下载的
不需要全部下载
所以速度可能会显得很快
那你们去在实际操作当中
可能会遇到某一个包啊
下载比较慢
这个都是挺正常的
就是看看时间啊
或者是这个挑一个不是那么常用的就好了
好我们拍touch安完
现在用pip安装几个Python的必要依赖包
OK我这里边显示其实已经安装过了
这个无所谓
再试一下
看这个嗯
也是安装成功的
然后我们就进入了第二个部分
我们要安装la index相关的包了
同样还是在这个嗯
conductive要在lame index这个看大环境下
然后直接复制命令就好了
确定吗
那个因为我们的教程
其实对于所有安装包的版本都是固定的
以确保我们同学可以顺顺利的就是呃跑完
所以大家呃在安装环境的时候
尽量还是使用我们这个原始的这个命令
就直接这样去copy
然后粘贴就好
不需要说我自己去手敲啊
或者怎么样的
然后呃也也别去尽量别去更换
就是相关包的版本
然后容易出现一些不兼容的问题
然后这些问题其实也挺难去debug的
嗯好了
我们这个基础的这个拉满环境
现在就构建好了
这个warning的话大家不用在意
这是因为我们在这个服务器虚拟机上用的是root
所以你每次安装它都会提醒的
不用搭理它
然后我们下一步就是要安装
我们这个实际上是进行了我们真正的开发部分
前面都是环境配置部分
我们下面要进行的是一个呃
嵌入模型的一个下载
因为很多同学都在国内
所以呃就是直接下载话
怕下不到
我们这里选用的是这个sentence transformer
这个模型来进行这个呃向量开源词
向量就是向量嵌入这个部分
然后相对轻量
对中文支持也好
这两个其实是真实使用当中比较重要的一部分
因为这个向量模型在嵌入和检索的过程
就是curry变成向量过程都要一直去使用的嗯
然后有同学其实可以尝试别的开源模型啊
像网易推的这些模型都可以去使用的
我们直接复制去
好的
我们现在先建一个文件夹
已经建立了这个叫lama index demo文件夹
我们以后的操作很多都在里面
然后我们创建了一个叫download HuggingFace的文件
我们在这个文件里面去复制这个内容
只粘贴保存
那保存的时候保存完毕之后
上面这个右右边的小点就会消失了
然后这个命令的主要内容是
我们先首先把我们哈根face的下载环境
变成中国国内镜像
然后我们调用这个命令行
我们在Python下调用命令行
去直接调用hugin face的命令行下载
去下载这个sentence transformer k直接就好了
就是复制命令转到拉玛demo
然后在看到环境下
然后去直接去Python download
就运行刚才那个文件就好了
然后他就开始下载了
这个我也需要一定的时间
然后如果是外国的同学
或者是你在不同的地方需要使用其他镜像
可以去这个链点
这个链接去看哈皮face的其他的一个呃
镜像去查看
就这边就已经下载完了
看一下啊
嗯其实他这个看我们是下载到了root model文件夹
所以这边只会出现一个model
新的文件夹
里面会出一个sence transformer的文件
就是这个就已经下载完成了
然后我们接着下载这个LLTK的一些文件
在使用的时候会用到它呃
去构建我们的这个呃呃向量database
就是向量库这一块
然后这也是说他其实正常会自动下载
但是网络问题我们怕下载不到
所以我们选用了这种国内的gt
从这个地方去进行下载
去进行这个数据的拉取
然后也是我们直接复制命令就好
然后你看他这边就是已经建立了这个
LTK数据的这个文件包
然后它会自动的去呃
去文件包里进行这个我们的这个token
ANIZER的这个那个拉取
然后下载这些过程
好我们把最后一个包解压缩
这个就完成了
也可以看到这个LTK下面
然后我们要进行的是
其实是我们呃我们要使用的书中葡语
这个1.8B模型的练
这个是应用
但这一块因为额模型相对体积比较大
所以我们会
我们已经把这个模型放到share的共享文件夹中
大家只要进行一个软链接
把它放到自己的这个目录下就可以了
同样要把这个模型放到model这个文件夹下
好运行命令我们去看一眼
OK我们已经有这个看一下啊
也有个int l m two chat one point ahb
这个模型已经下载好了
然后下面的时候
我们就要去进行的是一个模型的运行
我们先看一下
直接运行这个数
声谱与1.8B是一个什么效果
OK复制命令粘贴
嗯我们现在应该已经创建了一个叫la index
INTELM的文件
这个文件的地址就是在刚刚的la index demo
这对我们已经创建好了
嘿这lamy index intem这个文件
我们把这个所有的内容直接复制好
钻进来
OK这就是我们现在调用了lama index当中大模型
HuggingFace的HuggingFace大模型的一个组件
然后我们同时也调用了他的一个
聊天信息的内容
之后哦
我们选取的模型就是我们刚刚下载的1.8B
选用的这个TOANIZER
就是也是1.8B
然后我们这个在模型当中呃
相关的就是使用远程模式啊
这些关键词都选择true
然后这块我们用到的是呃
这个位置我们要要使用这个chat的模式
然后去让模型回答EXETA是什么
这是一个命令行运行的方式
OK我们保存和刚才一样
保存后它是没有黑点的
然后这样的话
我们就其实可以直接运行这个刚刚写好的文件
看看这个1.8B模型对于x ta是什么
这个问题他是怎么回答的
这个就是我们现在其实是正在载入
我们刚刚下载的那个1.8B模型的过程
然后嗯大家也可以看一下
我们这个显存使用大概就是在8G多一点
好了已经这个整个运行已经结束了
我们模型关于这个x tuna是什么的回答
大家看一下啊
挺好玩的
然后这个1.8B模型中
s to呢是款用于播放音乐的软件
支持这些这些这些就已经开始出现幻觉
胡说八道了
嗯下面我们就来直接开始建立一个RG过程
解决这个问题
让他认识eta是什么
然后我们首先要搭建的是
还要安装一些这个关于呃lama index当中
嵌入的一些部分的工具包
好的已经安装完了
然后我们要做的是把关于x tuna的这个内容
它的一个知识源下载下来
我们也下到辣妈index demo里面那个文件夹当中去
然后在这边操作当中
我们是把这个嗯
他的这个MD文件挪出来了
我们在拉on index当中
这个read me in文件
我们给挪到了这个位置
然后嗯这个时候我们要做的是把这个文件
按照我们刚才的流程
我们要做的是把这个文件这个read me
x tuner的read me给它进行向量化
变成我们的向量数据库这样一个过程
然后我们回到了my index文件夹
创建一个新的文件
叫做
Lama index r a g
我们所有的关于RAG的文件
就先都会放在这里
那这里边贴到这些代码
我先保存一下
这代码其实挺简单的
就是我们会使用la index提供的所有的工具
去读取我们的呃
刚才那个markdown文件让他知道是什么
然后把它进行向量数据化
是这样
首先我们先调一调看一下吧
就先我们先调用的是他的一个嵌入模型
就是我们刚才下好的这个sentence transformer
然后我们在设置当中把这个模型
我们的嵌入模型设置为这个模型的地址
然后我们再整个的HuggingFace这个调用这个呃
la index当中
调用这种hugin face的主键去调用我们的主模型
就是1.8B
就这个和前面的内容是一样一样的
然后设置当中把主模型设置成大模型
设置成这个哈哈1.8B
然后我们要用文档
这一步就是我们刚才做那个操作
就是我刚才在其实在上一部分的呃
理论讲解当中也提到了
就我们可以调用其实各种的reader
然后在lame index当成connector去直接调用
就是用load data这个去直接调用
去去获取我们的文件
然后我们把我们刚才的这个data的地址
就是我们下好的这个data的地址里边EXTINA啊
包括这个read me什么的
给它读取进去
告诉他在哪
然后我们用index你直接去采用它内置的一些方式
内置的默认方式吧
给它进行一个
就刚才我们这一些文档的内容
给它进行一个向量化
其实这个教程的我们的老师呃
这个这个这个comments写的都很详细
然后大家可以快速的读
我这就简单说一下
然后这边我们就直接调用的引擎
调用它这种curry引擎
然后回答cure引擎ta是什么就好了
嗯然后这边直接运行吧
好的我们看一下这回的答案
就是关于s ta是什么
它这回因为我们从x2呢
本身的GITHUB上下载了相关的文档
所以他这回能准确的识别sq呢是什么了
这里边就写到s tuna是支持微调大语言模型
然后数据处理这个东西在这啊
s ta是一个高效灵活全能的轻量化大模型
微调工具库
好下面是他的一些文档当中相应内容好
同时他也给出了这个文档的来源
这个就是我们说
为什么用RG可以帮助我们去那个溯源
去更好的去呃了解我们大模型的逻辑
就是它的内容都是来自于这个文档的
嗯这下面的内容没有什么大的问题嗯
这个其实我们就已经搭建了一个简单的
这个RG的一个demo
大家也看到了
它的效果其实还是很明显的
那下面我们再稍微呃加大一小点儿的难度
就是呃把这个命令行模式的给它
换成这种界面模式
那我们需要我们这回选用的是stream lit的这个
呃框架
这个工具我们先安装吧
要注意啊
这个也得是在我们的这个la index的counter
环境下运行
好的我们这个已经安装完了stream lit
然后我们这回要建一个app派的这个工具
看一下我们的app派刷新啊
他已经在这了
我们把下面的内容给他复制进去
其实这个就是用stream lit来进行
然后主要的部分还是在这个位置
就是如何把我们的相关模型载入进去
如何把我们刚才搭建好的那个呃RG系统
搭入这个呃这个页面当中
这可视化的一个效果
其他的话其实就是一些常规的关于页面设置啊
你像这种side bar button的设置啊
然后这种关于角色啊的一些提示词啊之类的
这些都是默认可以不用改动
然后如果大家有去更换这些的话
需要记得有去更换自己的嵌入的模型
或者是使用模型的话
可以在记得把这个位置改了就可以了
好的
这个文件保存
然后直接返回命令行运行就行了
好的这个已经搭建了
它这个端口是8501
刚刚的时候诶
在这儿我已经在8501的这块儿
8501和我在使用的也进行了一个透传
所以应该直接点击local就能看到啊
可以载入
他这个启动模型会稍微花一点点时间
也是这个loading checkpoint这个部分
所以啊大家不用着急嗯
等这边百分之百之后再看这个就好了
这边就载入结束了
嗯差不多了
应该诶好了
这时候他的第一个这个问答助手就问我
他说助手可以
我们先直接选用这个教程当中的吧
x tuna是什么
嗯嗯看他的回答
就是刚才他的命令行的是一样的
首先是直接答案
然后文件的内容巴拉巴拉嗯
大家就这样就可以去进行一下
自己去多多测试
多玩儿一下
你看我们现在整体得到的一个结果
就是这边是它的一个进行的一个文件分析
就是他是如何去调用的
然后如何去处理我们的问题
所以大家都可以去自己去多看一下
挺挺好玩的
然后操作上整体很流畅
简单就是呃就到这儿就结束了
我们其实就是如果排除我们整个安装的时间
我们其实用了很快的速度
搭建了一个RG的模型
那这个模型可以从无到有
从不知道的一个内容
然后变成这个内容的专家
这个都是可以的
然后就结束了