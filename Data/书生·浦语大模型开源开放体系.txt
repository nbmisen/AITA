哈喽大家好
我是你们的老朋友汪周谦
欢迎大家来到书生葡语大模型训练营
本节内容由我给大家介绍
我们书生葡语大模型开源开放体系
本节课的内容呢主要是偏人文课一点
帮助第一次参加我们课程的同学呢
能够快速了解书生葡语这个IP
以及让我们的老同学们能够快速了解一下
最近半年我们又有什么新的东西出来
具体的技术以及项目教程呢
在第三期会有其他老师手把手带大家讲解
那么这里呢我们之所以说是体系啊
是因为经历了这一年以来的努力呢
我们已经从大模型的全链路进行了打通啊
从数据的收集整理
数据标注到模型训练模型
微调模型评测
再到基于模型的agent rag搜索引擎
最后以及AI应用的部署
都进行了方案实现以及开源
所以确实也是蛮不容易的
首先呢让我们一起来回顾一下
书生璞玉的开元之路啊
从去年的7月6号开始
我们in turn l m模型呢
就是这个书生葡语达模型啊
7B的模型率先免费开源
免费商用
并且发布了全链条的开源工具体系
这个时候其实就包括我们说的x tune
微调工具啊
包括这个l m deploy的这些工具啊
然后到去年的9月底呢
就我们发布了这个英intern l m
20B的这个中量级的这个模型啊
这个7B的模型如果说只适用于个人的话
那么20B的模型
其实在一些中小企业以及一些科研机构呢
其实就是可以用了啊
可以用起来了
然后在今年的1月份呢
in turn l m2.0开源啊
它的性能超越了这个最新同量级的
这些开源模型
比如说他在7B量级下
它就可以性能达到其他一些开源模型的
这个20B
甚至是啊70B的这样的一个性能啊
然后今年在7月初的时候呢
INTELM2.5进行了一个开源啊
这个性能也获得了这个质的飞跃
那么这里呢就是每一代的这个英intern
LM模型呢的一个性能天梯
这里的in turn l m大家第一次接触的话
其实他就是你就可以理解成
他就是我们书生普语大模型的一个英文名称啊
但不过我们在说书生葡语开源生态
书生葡语开源体系的时候
它指的又不仅仅是in turn l m这一个模型
它指的是我们整个这个呃
基于上海人工智能实验室的这样一个
开源体系啊
好的
那我们接着来讲这个啊性能天梯
大家可以看到啊
在随着这个时间的发展
以及我们这个版本的迭代
我们不断的去与这个GBT的性能去靠近
然后在今年也是在2.5210B的这样一个
chat模型
性能与GBT40409
这个版本达到了一个齐平啊
所以还是作为一个国产的开源模型
还是非常不容易的
那么我们最新版的书生璞玉2.5呢
其实就是上一张幻灯片中的这个in turn l m
2.5啊
这个模型呢
它嗯在各方面呢
相较于先前的版本有一些比较质的飞跃
比如说他的这个推理能力是很领先的
它比上一代的这个INTELM2呢
它的性能提升了20%
那么说的是推理性能
说的是原生的推理性能啊
这里说的是不是说基于agent啊
或者一些外部逻辑的一些推理性能
而是说它模型本身的这种自身的这种推理性能
另外呢对于这个上下文
也就是大家比较关注的这种短期记忆啊
这种上下文的聊天记录呢
已经是达到了100万的这个级别啊
大家要知道GPT4O
最新的这个GPT4O模型
它也是只有128K的这样一个上下文啊
相较于GBT4O的话是十倍的一个量级
然后另外呢也有这个自主规划和搜索
完成复杂任务的能力
那么这里说的其实就是说呃
基于一些外部工具了
比如说我们后面会说到这个mind search
基于大圆模型的一个搜索引擎工具
强大的推理能力
解锁复杂任务
在整个开源体系的这个迭代发展过程中呢
核心的技术思想
主要还是一个不断的一个反馈的过程
比如说我们最开始发布呃第一版模型之后呢
我们不断的对数据进行过滤
进行智能评估
以及去指令生成一些辅助标注
做一些对齐数据
做一些预训练数据
从而获得第二个版本的模型
然后循环往复这个迭代过程啊
其实在这个里面呢
最终还是数据驱动的一个模型性能
数据质量驱动的一个模型性能
在这里呢高质量的合成数据呢
我们也使用了以下的这些策略
比如说一些基于规则的数据构造啊
包括这些代码呀
公式函数啊
数学解题啊
这些东西
其实它相当于是一种呃伪格式化
或者叫半格式化的这些数据啊
那么它基于一些规则进行构造
另外也基于模型进行一些数据扩充
比如说基于一些啊现有模型啊
或者说商业模型做一些数据扩充
比如说像这里一个简单的这种代码
那么给它增加一些函数
增加一些呃注释对吧
那么另外就是基于反馈的这种数据生成
这里呢就涉及到叫基于人类反馈的
这种强化训练啊
就是模型在生成多样化的东西的时候
你人类给它进行一个呃满意度的排序
从而通过这个排序来让模型在以后的版本中
更容易生成符合人类要求的这种答案
因为有时候我们我们去只做这种相似度对齐的
这种训练的话
它并不一定是符合人类满意的啊
就是一种主观的一种评价方式了
相当于是那么这里呢我还是要额外说一句
就这种基于反馈的数据生成
其实呃大家实操的时候
其实就最最最有痛点的就是他的标注
其实他的标注就是大家不知道怎么标注
现在我们书生葡语开源社区呢
也是在这个open data lab开源了
这个叫做label LLM的开源项目
大家可以通过label LLM
很方便的对这些NLP的任务
包括这种排序任务进行很方便的标注
领先的推理能力
这方面呢就是大家也可以简单看一下
这里叫相较于我们上一代版本
In turn l m two chat
7B的这个模型呢
在7B的这种体量之下啊
他的推理性能也是比上一代大幅提升
与其他同量级的开源模型相比
在各种评测数据集上
INTELM2.5也是有着不俗的表现
好这里是一个演示demo
就是让他去分析一下
这样一个可以说是一个markdown的一个table啊
那么可以看到
我们其实并没有做太多的提示词工程
在这个demo里面
大家可以看到这里也就只是说请回答啊
根据什么请回答
但是这个模型原声呢它就具备这种推理能力啊
然后就是这个1000000token的这个上下文
大家可以看到这个大海捞针实验
也基本上是这个全绿的状态啊
呃这里给第一次接触的同学简单介绍一下
什么叫大海捞针实验啊
就是当你给模型提供一段
非常长的背景知识的时候啊
模型它是否能够完美的去定位
这段超长背景知识中任何位置的任何信息
这就叫大海捞针实验啊
比如说这里啊
比如说大家看左边第一列
当我们给模型提供100000token的这个
背景知识的时候
那么模型能够几乎百分百的定位到文章中
任何位置的任何信息
就他能够百分之百的定位到这100000token的
背景知识中
任何位置的任何信息
那么随着背景知识它长度的增加
比如说逐渐增加到100万头
那么这个大海捞针的结果
他不可能永远是百分之百嘛
他也会逐渐开始遗忘
随着这个背景知识的长度增加
但是总体来说呢
像这种全绿的性能表现啊
已经是非常不错的了
这里一个demo演示的话呢
就是说我们直接把这个新唐书发给他了啊
这里面那那那那内容太多了
然后来让他进行这种额问题的
根据这个根据这个背景知识
来进行这种问题的回答
就是根据新唐书的内容
来进行这个问题的回答啊
那么像以前我们做这种任务的时候
基本上都是要做呃
做像rag就是RG的这种这种这种这种步骤的
就是先把新唐书拆分对吧
先做拆分
拆完分之后呢去做向量化
做完向量化呢
再把问题再做向量化
然后再拿问题的向量化与
去与这个已经向量化的这个新唐书去匹配
看能匹配到哪些分块
然后把匹配到的分块拿出来
然后再再再再去问语言模型对吧
这样其实我觉得来说
做一些简单的索引是没问题的
但是你让他去跨文档的去理解
跨文档的
去找出这种逻辑呃
我觉得是有问题的啊
就是原声的这种能够支持超长上下文的
这种这种这种功能
我觉得是
我觉得是未来是一个能够替代rag的
一个方向啊
那么那我不说所有场景啊
就是在在一些更更加general的场景下
我觉得是可以替代这种呃rag的
另一方面就是基于规划和搜索
解决复杂问题的能力
这里我们说的就不单单是说模型的原生能力了
其实里面涉及到一些
比如说搜索引
调用搜索引擎啊
调用这种返回API的结果
就是去模拟人解决问题的这样一个思路
比如说先对问题进行分析
那么这一步是语言模型本身完成的
分析成几个子问题之后呢
再去分步实现这些解决这些子问题
然后再判断是否需要调用外部的一些工具
比如说搜索引擎啊
比如说外部的数据库啊等等
然后最后将所有的结果汇总进行整合
然后再返回这个问题回答
那么这下面其实说的是mind search这个项目啊
就是说呃单词100加的网页浏览
其实这里就是一个mind search的一个项目
的一个截图啊
大家可以看到就是啊
待会儿待会儿在后面的课程中
在我们本期课本期实战营的这个彩蛋环节呢
会带着大家来做这个my search
这个也是最近不是GBT出了一个那个
搜索引擎的那个东西嘛
然后还要等加入什么wait list什么之类的
搞得就很不爽对吧
呵呵咱们就直接给他开源出来
这个体验还是很好的
这个你只要把它搭起来了之后
日常比如说一些调研工作啊
搜索工作其实慢慢的你就离不开这个东西了
那么书生普与开源模型谱系呢
我们从这个模型量级上来说
主要一个是1.8B的这个10亿参数的模型
这个模型其实绝大多数应用的话
你可以把它放到端侧去
比如说在在手机上呀
在一些边缘设备上去做一些东西
总的来说也可以拿它在笔记本上面跑
就是大家去在本地做学习的时候
这个模型微调啊等等
另外一个就是7B的参数
也就是70亿模型的这个参数量
它的性能也是不俗的啊
对于一些轻量级的研究和应用也够
也能够提供一定的支撑
那么更为强劲的一个
也就是其实一些生产环境可以用的呢
那么就是200亿参数的这个模型
那就我就我自己来感觉呢
我我我是觉得200亿参数的
也就是20B的这个模型啊
它才真正出现了这个叫涌现的这种
这种这种这种现象
那这个是我自己感觉啊
就你7B的去做微调
去做二次实验的时候
你会觉得诶他好像还是只是基于训练数据
在在在在在在在做一些检索的感觉
但是20B的模型
你很明显能感觉到他是有涌现的
这种现象出现的
所谓涌现就是说你没见过的东西
他也能够进行回答嘛
那么102B的呢
那么是没有开源的啊
那么从模态来分呢
首先是in turn l m x composer
这个图像文本多模态的模型
另外呢就是intel l math的模型
它主要是针对数学的一些场景
包括这个前前段时间高考嘛
出了一个INTELM文曲星的这样一个模型
专门是用来做一些高考题目
当然了
我们这个就是主要是做科研目的的
那么这里就是书生谱与全链条
开源生态的一个总览图啊
大家可以看到从数据到预训练到微调
到部署到评测
到应用全链路
所有的工具都给大家开源了啊
实现方案都给大家弄弄出来了
从数据方面呢
这个在也开源了一个预训练的语料库
书胜万卷
当然这个对于个人用户来说
其实除非你自己做科研啊
对于个人用户来说
其实用不到这个东西
当然了
在这个书生万卷上面呢
大家也可以找到很多这种子领域的语料库啊
这这当然个人是可以用的
都开源了啊
大家可以去书生万卷的官网去download下来
然后另外预训练框架就in turn evil这个框架呢
那这个当然就是大家如果有企业用户的话
可以去尝试使用啊
他主要是做预训练模型的
以及这个预训练模型的一些迁移学习等等
那么我们个人比较常用的
包括一些小型企业比较常用的呢就是微调部分
这个微调框架x turner第一期
第二期也是也是我了
我给大家讲的
这里呢就是一个比较方便的一个微调框架
大家不懂代码的话呢
其实也也是可以快速上手的
那你微调好模型之后
你肯定就要部署嘛
你你总不能让你的客户在在LINUX终端里面和模型
对话嘛
对吧
你部署的话呢
就是我们也出了这个叫lm deploy的
这个部署工具
这里面呢就是我们在各方面性能上
就是超过了国际主流的这个
VLLM的这个推理框架
所以说也希望大家企业用户可以用起来
我们这个l m deploy
然后评测的话就是open compass这个四南评测
另外就是应用方面
比如说茴香豆的这个rag工具啊
这个其实相当于是一个rag框架
给大家呃快速的搭建自己的rag应用的
minor u呢就是高效处理文档的解析工具
就是大家在这一前面这一步做微调的时候
其实很多数据是来源于你的文档
PDF啊
word呀等等
那minor u可以就可以帮你快速的解析
这些P的UF文档
转化成我们需要的这种纯文本格式
里面也继承了一些很多OCR的工具
然后这个agent的呢就用这个legend工
legend这个智能体框架
可以快速的搭建自己的agent应用
另外就是搜索引擎的这个mind search
刚才也给大家看了
所以整体呢从数据到预训练到微调
到部署到评测到应用
所有的方案都给大家开选开源了
然后并且呢与这个嗯开源社区的这些工具啊
进行了无缝的衔接
就是INTELL模模型啊
在这些开源工具上都是支持的
你比如说像欧拉嘛
现在也可以直接拉取英特尔LM2.5了对吧
那么我们拆分开来来讲
在数据方面呢
模态已经有了30多个模态数据集
有7700多个数据集
总体数据大小已经达到了180tb啊
其中包含有60亿的图像啊
1000000000000token的这种语料
2万小时的音频
8亿片段的视频
以及100万个这个3D模型
另外就是这里就是这个minor u和label LLM
以及label u
这个minor u就是一站式的开源
高质量数据提取工具啊
大家就是它可以直接从PDF网页
电子书这些里面直接去生成一些纯文本内容
就把它提取出来
因为大家知道PDF里面的结构其实是非常的复混
乱且复杂的对吧
它跟TXT跟word不一样
所以这个确实工作量还是不小的
然后这后面这个label LLM以及label una
那我个人是超级无敌喜欢的
就是label LLM
我此时此刻其实在我的呃
在我的团队就在使用啊
就是去做一些这个
问答对以及图片文本的这种多模态
问答对的标注
当然说如果你数据量大的话
是完全可以借助这种AI去辅助标注的
这里面也是支持的
就是你把AI辅助标注的内容导进去就可以了
包括这个label u也是传统图像的这种分割分类
以及这个detection的这种任务
包括它也支持视频标注
其实市面上视频标注的这种开源软件非常少
能够做到好的费用吧
不多的啊好的
那么关于这个预训练
in turn evil呢
这里主要是
相较于其他的一些预训练的一些框架的话
主要是进行了一些显存优化呀
包括一些分布式训练
以及分布式训练之间的一些通信的优化
就是让原先训不起来的东西
跑不起来的东西
现在能够跑起来
能够降低你的这个硬件要求啊
这个对于企业来说
其实非常节省成本
以及提高效率的这样一个一个一个东西
另外微调XTNER呢
他也是这个微调框架也是支持了
比如说市面上各种其他厂牌的这种开源模型
然后他的任务类型包括增量预训练指令微调
多模态微调以及对齐
这个X图的微调框架都是支持的
数据格式呢
就是说呃就兼容一些已经开源
并且用的比较多的一些开源数据集的一些格式
其实它内部就是都转化成统一格式
我们自己在做的
如果是自定义数据集
你直接做成就是X通的统一格式就可以
那么训练引擎是基于open m m live的
MMN进引擎啊
优化加速使用了这个flash attention
这个flash attention它直接内置就开启了
都不需要你手动去去做什么东西
以及deep speed zero
Pytorch f s d p
以及sequence parallel等优化加速方式
支持的算法呢主要是两点
一个是q la的微调
还有的是LAURA这两种算法的微调
其实现在市面上不管是科研还是企业产品
做微调
基本上用的非常多的都是这两种方法
然后全量参数微调也是支持的
当然全量参数微调
大家如果在自己的电脑上
个人消费级电脑上就啊不要试图去尝试了啊
包括说1.8B的
1.8B的
你做全量微调
个人消费级计算机也是跑不起来的
但是它的过程是一样的
就是说无非是呃
因为XTNER这个微调框架把它全部打包好了嘛
其实你学习的话去做这种Q62微调
完全没问题
他的这个嗯整个的操作过程啊
工作流程跟这个全量参数微调没没有区别
那么这里也就是x ta的一些评测啊
就是用x2的微调
和用其他的一些这个lama factory啊
这个微调框架来微调他的一些评价
比如说原来用lama factory
跑不起来的一些大参数模型对吧
那有x ta我们就能跑起来了
这主要就是进行一个显存的一个呃一个回收啊
不要不要在这个反向传播的过程
浪费它的显显存占用
这里同样就是说呃
X通路的一个零显存浪费的一个偏好
对齐训练方案
那你模型微调好了
你不就要评测了吗对吧
Open compass
这个呃思南评测体系呢
已经广泛应用于头部
大模型企业以及科研机构了
同时呢
他也是作为这个大模型国
评测国标的这个主要单位啊
并且也获得了meta官方推荐
是唯一
官meta官方推荐的这个呃国产大模型评测体系
开源社区最完善的评测体系之一
超过了100家的这个评测机
五以及50万加的这个题目
open compass呢是工具基准榜单三位一体啊
它提供高时效性的高质量评测集
支持高效评测
支持能力分析
全站式的评测工具
最后呢发布权威榜单
东西行业趋势
open compass致力于构建科学领先
公平的大模型评测体系
携手行业助力通用人工智能发展
那么你模型评测好了之后
你就要进行部署吗
部署我们也有这个l m deploy部署框架
大家也可以看到
这里已经支持了非常多的这个
开源模型的部署啊
这个VLLM只支持这有限的几个对吧
这个这个l m deploy
支持了这个更多的这个国产大模型
推理接口呢有PYPYTHON的推理接口
有restful接口
也有GRPC的接口
量化呢有这个只基于这个权重的量化
也有kv catch的量化引擎
支持turbo mind的推理引擎以及PYTORCH推理引擎
然后服务的话呢就是有类OpenAI的服务
然后并且前端也有gradual的方案
以及TREATON的推理服务
嗯这里就是l m deploy的一个性
推理性能的一个对比啊
就与VLLM的一个对比
各方面都是领先VLLM的智能体方面
就是刚才说的那个
如果我们很多任务只交给模型去做的话呢
就是它还是有一些局限性
比如说模型没法获取最新的知识
并且模型在一些计算能力上
一些呃数学运算上它还是呃不是非常可靠的
所以呢就要对大模型进行这种智能体
框架的这样一个构建
让它与外部工具进行交互
从而提高我们输出的这样一个可靠性
智能体主要是这个legend框架
legend里面呢它支持了react框架
支持了review框架以及auto g b t这三种智能体框架
那智能这三种呢
就是学术界三种比较主流的
这个智能体的一个构建方案
同时legend也支持多种大语言模型
你这里面想调用本地的INTELM也可以
想调用GBT也可以啊
都是支持的
这里是两个demo
一个是代码解数学题的能力
就是这里他会你你你你问这个数学问题
他就会去调用Python的这个解释器
然后去呃实现你的这个问题
用Python代码去实现你这个问题啊
那他准确度就很准了
你要么代码就跑不通对吧
就是把数学问题转化成语义问题
零样本放化的这个demo呢
就是多模态模型AI工具的一个调用
比如说他说describe this image
让让模型去描述这张图片
legend框架呢它就会哦
第一步是去调用这个image description的这个插件
这个插件那肯定是一个多模态模型了啊
对图片进行一个生成一个描述
哎第二步它使用叫text reader
就是文字转语音的这个工具对吧
然后再生成一条语音
那么最后是实现这样的一个效果
用语音进行回复的一个效果
这里呢就是我们最新开源的这个叫mind search智能体
其实它就是一个搜索引擎
但是呢它是基于AI以及基于这个呃
搜索结果的一个搜索引擎啊
这个在彩蛋环节中会给大家详细讲解
大家也可以看一下这个demo
你提出一个问题之后
他先他会规划
他会规划解决问题的这个思路
并且每一步的思路右边会显示出这一步思路
他正在干啥
比如说哦他现在思路已经到1900年8月
那这里他就在搜索
1900巴黎奥运会的一些相关东西
以及思考结果
而紧接着他现在又在思考1924年的这个
他就把整个模拟人脑的这个思维逻辑
思维路径啊给你可视化的展现出来了
我觉得这个设计真的非常的非常的绝啊
然后当他所有的思维结果完成之后
他就会总结一个最终的一个结果和报告
那你想想这个过程中他搜索了多少网页
对不对好
那么下一个就是茴香豆的一个企业级知识库
构建工具
其实它就是一个RG的一个工具
那么它不仅仅支持这个检索检索
增强生成RIG
它也支持这种啊知识图谱的啊
kg这种支持图谱的这种可解释的行为
茴香豆呢也是开源免费商用
并且呢应用RAG和知识图谱呢构建了啊
已经构建了1500多个知识库
以及500多个用户群
领域知识呢支持七种文档格式更新就立即生效
不需要重新部署以及更加安全简单便宜
扩展性强等
所以这里呢还是非常的非常的开心啊
就是从去年一直到今年
逐渐看着呃
书生谱与大模型的这个整个开源体系
逐渐的完善
逐渐的构建这样一个从头到尾从数据到训练
从科研到应用
从开发到部署的这样一个完整链路的
这个开源生态啊
真的非常非常了不起啊
那么我们书生葡语大模型实战营呢
也是呃顺利开展了两期
这是第三期
往期中也是有非常多的学员开发了这种呃
自己的这个毕业项目啊
那么我们将持续以这个开高质量的开源赋能
创新好的我们这节课的内容就先讲到这里
我是汪周谦
我们下次再见