Hello
大家好
我是安红俊书生
葡语社区贡献者
来自中国电信人工智能研究院
今天为大家带来的课程是
LMDP量化部署进阶
本次课程呢主要分为五个环节
第一章我们介绍一下使用m delight部署大圆模型
第二章至第四章
介绍大模型部署中三个比较核心的技术问题
分别是缓存推理
也就是k v catch大模型量化以及大模型外推
第五章为大家介绍一下IMDEPY的最新版本
支持的一个新特性
Function calling
首先我们先来介绍使用m deploy部署模型
关于什么是模型部署
在软件工程中部署
通常指的是将开发完毕的软件投入使用的过程
具体到人工智能领域
模型部署是实现深度学习算法
落地应用的关键步骤
简单来说
模型部署就是将训练好的深度学习模型
在特定环境中运行的过程
比如将已经训练好的模型部署到生产服务器
包括但不限于CPU服务器
GPU服务器或者是NPU服务器
甚至要部署到一个算力集群
或者是部署到端侧
比如说移动机器人
手机等设备
I will depy
提供了大L模型部署的全链条开源工具
能够十分便捷地帮助用户部署自己
大模型应用在接口方面支持Python接口
这允许我们通过import导入包的方式
将推力引擎引入项目进行快速的集成
同时也支持常见的网络服务接口
方便我们将项目和推理引擎解耦
通过网络调用的方式来访问大模型服务
在量化技术上
MDEPY支持常见的weight only量化和kv catch量化
能够在精度损失可接受范围内
在相同的硬件条件下
提供更快更长上下文的推理服务
在底层引擎上
MDEP自主研发了TURALM推理引擎
实现了一套高效的扩大算子进行推理
但是这道算子
仅支持拉玛或者是类拉玛系列的模型
对于想要实现自定义算子的用户
MDEPY也提供了一套PTCH的推理
后端使用上更加灵活
但是效率相对会低一些
MDEPY也提供了服务化封装
可以便捷的对外提供类open i的接口服务
也可以通过GRADU组件与当模型交互
MMDEP目前已经支持了许多主流的开源模型
包括lava
英特尔M千问等语言模型
还有lava等多模态大模型
MDEPY推理性能引擎
业界图中呢是在A100上分别测试的
VM推理框架和MDEPY推理框架的吞吐量
可以看到MDEPLY明显优于VLM
下面呢我们将依次介绍大模型推理中
几个比较核心的技术点
首先介绍大模型缓存推理技术
大模型是一个decode n类的模型
核心是transformer的decoder架构
该架构中有一个核心的算子
也就是注意力机制
在注意力机制中
对于输入的张量X
我们首先要通过三个线性变换
将其转化为query k还有value
其中呢query和K做内积来计算注意力的得分
在与value进行注意力汇聚
对于输入阶段
可以理解为我们与大模型进行对话过程中啊
输入问题的这么一个阶段
我们的问题是一个序列
这个序列中包含多个token
这些token是一次性输入大模型的
我们在计算注意力机制的时候
会同时拿到所有的query key
还有value
这不会存在任何的问题
这个阶段呢我们叫做预填充阶段
也就是PREFEELING
但是在大模型生成回答的过程中
也就是generation阶段
token是逐个迭代生成的
对于每一次迭代大模型接收一个新的X
我们需要把这个新的X以及历史上所有的X
经过wq wk wv之后得到QQV
再进行注意力的计算
这里呢就会发现一个问题
就是除了当前最新的X计算得了QQV之外
其他历史的QKV我们历史上是计算过的
这次我们又重新计算了一遍
而且对于这次迭代
我们其实只关心新生产的那个token
以前的那些token是没有任何用处的
那我们就不妨做一下优化
对于新的迭代
只输入新的X计算
新的QKLOV历史的K和V我们缓存起来
然后只把新的Q和所有的K进行注意力
分数的计算
在与所有的V进行注意力绘制来得到新的Y
我们可以看一下
英特尔M官方是如何实现这件事的
以下这段代码是英特尔M的PYTORCH代码
然后这个forward呢
是它计算attention的这么一段过程嗯
我们可以看到这个函数的输入参数
这个hidden size就是我们刚刚提到的那个X
就是新输入的X
对于PREFINI阶段
这个hidden states将会是一系列token的embedding
对于深入阶段
Get in the states
就是最新的那个X所对应的这个embedding向量
这个past kv在PRINFINITY阶段它是空的
对于GENERNUATION阶段
我们可以传入历史的KV
来进行一个attention的计算
然后这个输入进来之后啊
我们看到啊
这个其实是多卡张量并行下一个特殊的处理
它的本质上呢就是进行了一下这个操作
就是把我们这个hidden states
也就是QQV啊
他给合在一起了
经过这个WKQV层
去分别计算我们的QK还有V
得到我们新的这个qq v states
然后这里呢我们就会把这个QQ还有V啊
把这个统一合成的这个张量把它给分开啊
得到QK还有V再往下啊
这是刚刚得到的那个QQV
这里呢就是去计算我们的这个旋转位置编码
计算完了这个cos还有sin之后啊
把它应用到我们的这个Q还有K上啊
得到我们经过旋转位置编码之后的Q还有K
然后这里就是我们最关键的这个KV
catch的环节了啊
那如果说我们这个past kv这个值
它它是非空的啊
也就是说这一次输入我们传入了历史上的K
还有V
那么我们就需要把历史上的KV
和我们当前的KV进行一个合成啊
具体是怎么做的
是由我们这个past key value这个函数来实现的啊
通过这么一个update操作
我们把当前啊新的这个K还有V传入之后
这个函数就会把这个新的KV和历史的KV
进行一次拼接
拼接完了之后啊
再返回给我们当前的这个K还有V
那么拿到我们这个K和V值之后啊
后面呢就是进行这个注意的计算啊
这是那个矩阵乘法
然后后续再进行后续的过程啊
这个呢就是这个英特尔M官方基于Python
对于这个kv catch一个实现
那么i am dep在实现这个kv cash的时候
它在逻辑上跟刚刚的PYTCH代码是类似的
但是为了这个实现上更为高效
以及对内存更为有效的利用
MDEPAY实现了一个kv catch管理器
他负责这个catch的更新维护
它甚至可以在显存不足的时候
把当前不需要使用的kv catch
由显存换入内存啊
这非常类似于操作系统中
这个分页内存管理的机制
当然这一套复杂的机制
对于我们使用的用户来说是无感的啊
我们只需要告诉这个MDEPAY
你最多能使用多少显存啊
当然这里要注意啊
就是IMDEP它使用的是一个预先申请的机制
也就是说假如你告诉md ply啊
你最多比如比方说你最多只能使用8G的显存
那么IMDEPAY在启动之后
无论当前这8G显存是否真的全部用到了
他都会把这8G的显存预先申请啊
这个目的呢是为了减少运行时
因为申请释放内存所消耗的时间啊
在前期啊一期二期的课程中
在学习模型量化的时候
有些学员就反馈说啊
我在量化前
我发现这个模型在运行时候占用的显存是8G
我应用了这个模型量化之后
它占用的显存还是8G
那这不是没减少吗
啊其实不是说我们这个量化没有起效果
模型权重所占的显存一定是减少了的
那剩下的这部分显存就是更多的这部分显存
其实呢是分配给了我们这个kv catch啊
也就是意味着你在推理的时候
可以去这个推理更长的上下文啊
那我们去如何去告诉MMD快
你最多能占用多少显存呢
啊我们是通过这个catch max entry count
这个参数来调节kv catch占用内存大小啊
这个参数设置的是占用剩余显存的比例啊
这个呢就是我们在这个Python代码当中啊
当然在后面这个动手实践环节上啊
有有老师会带大家实际来实操这个过程
如果说我们把这个参数设置为0.2的话
那就是告诉MMDEPAY
你可以使占用的显存大小
是在加载完模型权重之后
剩余显存空间的20%好
那我们可以看一下这个MD回来的底层
它具体是一个什么样的实现逻辑啊
当我们把这个呃catch Mac entry count
这个0.2传到MDEPAY之后啊
在底层呢它首先它就会去计算啊
剩余的显存
我可以去申请多少个这个kv catch block块
当我们把这个block count
这个参数传到我们这个m d bi底层之后啊
可以知道现在我们这个数是0.2
他会先去调用这个get block count这个函数去计算
我能去申请多少个kv catch block块啊
他在计算的时候呢
首先呢就是先要去计算一下啊
当前我们这台机器剩余了多少的显存大小啊
是多少G对吧
计算完了这个free之后
这个free取会去乘以我们传入的这个0.2
这个ratio啊
就是说我们现在能占用的显存空间
就是剩余显存大小的20%
这20%的显存大小
再去除以单个block所占用的内存空间
就是我们这个kv catch
能具体能使用多少BLO块啊
传过来之后
那么后续的这个代码逻辑
就是去申请对应的显示空间啊
然后再去做这个kv catch这个管理
第三部分
我们来学习一下这个大模型的量化技术
量化是一种将传统的表示方法中的浮点数
转化为整数
或者是其他离散形式的这么一种技术
以减轻深度学习模型的存储和计算负担
关于为什么要做量化啊
最直观的
它能够减少我们这个模型权重所占用的体积
比方说啊我们这个最新的拉玛三
拉玛三
最新推出的这个最大的模型是405B的
一个超大的模型
如果说我们是采用传统的16位
浮点数的存储方式的话
它的每一个参数啊
因为是16位
它的每一个参数就需要占用两字节的显存
那么对于我们这个405B的模型
它就需要去占用810G的显存啊
这是一个什么样的概念
要知道我们这个普通的家用笔记本电脑
比方说这个带独显的游戏本
显存一般最大也就只有8G
但是它却需要810个G的显存啊
对于公司来说啊
就算我们有这个巴卡A100的服务器啊
或者是巴卡A800的服务器啊
并且呢我们还需要是那种满血的
就是80G显存的那个版本啊
这个巴卡它不过总共也就640G的显存
距离810G还差200多个G啊
也就是说
如果我们用传统的这个FB16方式
来加载这个权重的话
我们单机八卡的这么一台服务器
我们我们连这个权重我们都放不下
但是如果我们能把它这个权重量化为
四比特的一个整数的话
也就是说我们每一个权重由16位转为四位
存储啊
这个权重的体积就会降为原来的1/4
比方说啊405B的这个拉瓦三
它原本需要占810个G4分之一啊
就是int4量化之后转为原来的14
我们现在只需要202个G
那202个G
对于我们这个单机八卡的服务器来说
就是绰绰有余了啊
剩下的显存我们可以分配给kv catch啊
去支持更长的这个上下文的推理
或者说我们去做一个高并发的支持啊
我们一台服务器可以同时去服务多个用户
所以说量化技术所带来的这个性价比啊
还是非常可观的
那么这个量化的思路
它的最核心的思路也是非常简单的啊
就是对于我们原来的这个浮点数的这个区间
我们把它做一个线性的一个映射
映射为一系列整数
我们以这个INT8为例啊
INT8它就是八位二进制数
八位二进制数所能表示的这个整数范围呢
就是从0~255
一共是256个数
那么我们就可以把原来的这个浮点数的范围啊
按照最大最小值把它平均分成256份啊
然后按照这个大小到相对顺序
进行一个线性的映射啊
把每一个区间的那些数
就把它线性映射到一个对应的整数上
那个这就是我们这个量化的一个
通用的一个思想
那么我们这个按照量化的对象
我们这个量化可以分为这个K对
kv catch的量化
对于模型权重量化以及对于激活值的量化
kv catch啊
很好理解
就是我们在第二章中所说到的那个kv catch呃
通通通常来说他会跟萌新选手保持一样啊
之前是LOB16
那么kv catch
我们一般也会去采取
采取一个LP10的存储方式啊
当然为了节省空间啊
支持更长的上下文
我们可以对q v catch也进行一下量化啊
印量化为int4或者INT8等等都可以
那还有就是模型权重也很好理解啊
就是模型权重本身我们可以进行一个量化
关于这个激活值啊
关于激活值
很多初学者其实对于激活值本身
这个概念不是很理解啊
因为这个这个这三个字本身
它具有非常强的一种迷惑性
因为一说激活啊
包括我在初学这个时候
我就把它联系到我们这个激活函数上去了
就根本不太理解这个激活值到底是什么意思啊
其实非常简单啊
三个字非常看
虽然看起来非常高大上
但实际非常简单
比如我们拿这个线性层举个例子啊
线性层它的数学本质就是Y等于WX加B啊
核心就是那个WXW就是模型的权重
X就是激活值
就是我们需要把这个X跟这个W进行相乘
W是权重
我们可以对这个权重进行一个量化
也可以对与I与这个W相乘的那个X
那个X就是激活值
我们也可以对激活值进行一个量化啊
这就是激活这量化的含义
那么除了按照量化对象分
我们还可以按照这个量化阶段去分啊
分为这个量化感知训练啊
量化感知微调啊
还有这个训练后量化
其中这个训练后量化
就是指的是我们再把这个模型训练好之后啊
我们可以通过一些简单的数据集标定
或者其他一些方式
不需要重复训练就可以完成这个量化的过程啊
这个策略通常是比较优的
那么前两种再把这个模型训练完之后
通常我们还需要采取一些额外的微调步骤
来进行这个量化
那么在我们这个生产实践中
通常是不会采取的
通常是采取这个PDQ的方式
那么LMDEPL呢
I m depay
它主要采取的就是这种训练后量化的这种策略
并且支持对我们这个k v catch以及模型权重
进行一个量化啊
那首先来看我们这个k v catch量化MDEPY
它使用他采取的方式呢是一种在线的kv catch
int4或者是INT8的这么一种量化方式
那么得益于呃这个优秀的kv catch管理机制
MDEPY在这个kv catch量化上
他可以做到一个per head
per token一个非常细的这么一种力度啊
与FP16相比啊
E的四
E的8K v block数量分别可以提升四倍和两倍
这意味着我们可以支持更长的上下文
以及更高的一个并发的吞吐
那么在精度上
INT8的在线量化方式几乎是无损的啊
可以看一下下面这个表格
INT8几乎是无损的
int4呢是略有损失的
那么大家在使用的时候
可以根据自己的需求
对于精度损失的可接受程度
来选择合适的KV开始量化方式
那么对于这个权重
对于权重MDEPY采取的是基于AWQ算法的
W4ALOAL6的这么一种量化方式啊
什么是W4A16啊
这个W就是那个权重weight啊
就是对权重进行一个四比特量化
A就是那个activation
就是刚刚说的那个激活值
我们对激活值值不做量化啊
对激活值不做量化
也就是说在MDEPY量化的策略当中
我们只是对权重进行一个四比特的量化
然后用四比特进行一个存储
但实际在计算的时候
我们还需要把这个四比特的权重反量化为
FP16的浮点数与这个激活值去进行计算
这里引申出了一个问题
就是呃之前其实普遍的这个想法呢
就是说我们为什么要对模型进行量化
进行量化
可能是为了去调用这个硬件底层的一些
定点数的算子啊
比如说我们现在最新的这个英伟达显卡
它都是有这个INT8的这种计算单元的
有这种专门的计算单元
而且算力上来说
比我们这个LP16或者FP32的计算单元
它的算力是要更高的啊
所以有很多人就啊非常机械式的认为啊
我们就是因为对模型进行了量化
把这个浮点数转成了一个定点数
然后去调用这个呃算力根号计算单元
所以能对这个模型进行一个加速啊
这种说法其实是比较片面的
可以实际看到啊
就是MD5I它在实验过程中
这个四比特它仅仅是用来存储的啊
就是来节省我们这个存储的空间
但是在计算的时候
我们非但没有用这个整数的计算单元
我们反而还需要把这个整数反量化为
一个浮点数
在浮点数的计算单元上进行一个计算
但是最终他还是能起到一个啊比较好的
这么一种加速的效果
就是性能呢是纯FB十六二十四B以上
为什么呢
因为我们这个transformer模型啊
它在底库
这它在这个decode阶段就是generation阶段
它是一种超高仿存的这么一种操作
也就是说大模型在实际推理的时候
我们这个系我们这个计算瓶颈本身
它并不是在我们这个计算上
而是在保存上啊
因为大家如果去看一下官方的那个
就是对于这个呃就是这个显卡的这个数据带宽
可以去看一下
这是一个非常明显的瓶颈
我们对这个权重进行一个四比特量化之后
他把在推理过程中所需要的数据量
通信所需的数据量缩小为原来的1/4啊
这就减少了这个IO
减少了这个IO的操作
从而起到了这么一种加速的效果啊
这是这个m d pi量化方案
那我们这次课程啊是一个进阶课程
所以说关于这个AWQ算法的底层原理
我觉得我们也有必要去讲一下啊
这个算法具体是怎么做的呢
啊有这么一个学术界有这么一个核心观点啊
并且是通过历史的清晰工作
已经证明比较正确的这么一个观点
就是怎么说呢
就大模型我们这个权重很多
比如405B那个模型
他已经有405B0啊这么多的权重
但是这些权重并不是同等重要的啊
仅有0.1%到1%
这么一小部分的显著权重
对推理结果影响是比较大的
那么现在啊就有人说了
我们能不能有一种方法啊
把这0.1%到1%的小部分
证明一部分选中选中啊
不是说这一部分显著权重
对推理结构影响比较大吗
OK那么这一部分显著权重我们不做量化啊
宝钗vb16对于其他不那么重要的啊
99%的那些权重
进行一个低频率的量化
从而可以大幅的去降低这个内存的占用啊
这个理想非常的美好啊
那么问题就来了
我们如何选出这些显著权重啊
这里我们有三种方式
第一种呢叫随机挑选
听天由命啊
这种显然是不太合逻辑的
虽然说我们这个模型权重当中
只有0.1%到1%是重要的
但是他肯定也是一些特定的权重啊
肯定不是靠这个随机挑选而去选出来的
第二种方式啊
我们可以基于这个权重的分布去挑选
就是从直觉上来说
我们有这么多权重
到底哪些权重才是重要的呢
直觉告诉我们啊
就是越大它越显著啊
好像确实应该是这样子的对吧
但是实际上我们在这个通过这个作者通过实验
打破了我们这一直觉可以看到啊
嗯作者选用了一个指标叫困惑度啊
这个指标越小代表模型精度越高
OK他评估了这个1.3B
1.7B还有13B这么三种规模的模型
那么对于第一种方式就是随机挑选
可以看到随机挑选0.1%
所得出的这个PPL指标
分别是它它它那这个三个数
相比于我们这个baseline啊
baseline就是原来不做量化
IOP16下这个困惑度是十四十十
通过这个随机挑选啊
你会发现哎
他这个重合度一下子就飙到了一百一百多啊
这后面呢这个6.7B是20多
这个是40多
所以看到这个随机挑选好像不是特别合理的啊
如果是随机挑选1%的话
这个效果也是比较差
3%也很差啊
所以说这个随机挑选
并不是一个非常合适的方式
那么关于第二种就是基于权重分布挑选
就是说我们拿到权重之后
首先对取所有的权重进行一个排序啊
然后把前0.1%作为一个显著的权重
看看行不行
但是结果啊可以看到通过这个按照大
按照这个权重大小分布进行挑选
所得出这个困惑度跟随机挑选差不多
这也就是说
如果说我们按照权重大小去挑选
这个显著权重的话
在效果上跟随机挑选是差不多的
所以说这种方式也是不行的
那么作者提出了一种全新的思路
我们应该去根据激活值的分布去挑选
就之前说了嘛
比如对于这个Y等于WX这么一个线性运算
W是权重啊
刚刚说的基于权重分布挑选
就是基于刚刚我说的那个W的大小进行挑选
实际上我们应该根据这个激活值这个X大小
去进行挑选
作者进行了实验啊
如果我们把这个激活值的分布进行一个排序啊
把百分之前0.1%锁定的那些权重啊
作为一显著权重的话
哎他的这个困惑度可以看到
基本上就降的很小了
跟原来这两种方式提起来
这个优势还是非常明显的
OK那么经过刚才的这个分析以及实验
我们的思路已经非常明显了
我们就是要基于我们这个激活值的分布
来去挑选对应的显著权重
对显著权重保持LV16
对于其他的权重进行int4量化
然后作者为了避免在实现上过于复杂
所以他在挑选权重的时候
他并不是在元素级别上进行挑选的
而是在通道的级别上进行一个挑选
什么意思呢
啊就比如说对于我们这个输入X这个张量啊
呃这个方向上啊是secret是sequence length啊
也就是说第一行是第一个token
第二行是第二个token
第三行是第三个token
然后每个token它都是一个embedding向量啊
这个维度上它是一个hidden size
那我们在这个通道级别上进行挑选
就是对于每一列进行一个挑选啊
怎么做呢
就是对于我们这个输入的XX有很多
我们分每一个通道啊
就是分每一列我们去求这个绝对值的平均值啊
对于平均值较大的一列啊
就是对于平均值较大的通道
我们我们视为啊就是显著通道
那么对于这个显著通道所对应的模型
权重的那个通道
这里可能比较拗口啊
就是什么什么意思呢
我们现在认为啊
就是说根据我们这个激活值的分布
我们认为这一列是比较显著的
那我们在进行矩阵乘法的时候
大家可以看一下
就是我们在进行矩阵乘法的时候
通常是拿第一行啊去跟这个权重的这一列
进行分别的相乘
然后最终相加
然后拿这个的第二行再跟它进行一个相乘相加
那么最终从效果上来看
这一列激活值在进行运算的时候
实际上对应于权重的这一行啊
就是这一列只会跟这一行去做运算
那我们发现了这个显著的激活值之后
它所对应的权重的这一行
这一行啊就是这个显著权重
我们把这个显著权重挑出来之后啊
你看它我们就保持这个LP16的精度
对于其他的这些权重
我们就进行一个低比特的量化啊
这个呢就是AWQ的这么一种思路
但是啊新的问题随之而来啊
理想很丰满
现实很骨感
大家可以思考一下
我们刚刚的方式看起来非常好
但是我们在实现的时候
难道真的要对于显著权重进行一个四量化
非显著权重保持FP16吗
或者有这么一个问题啊
就是说我们刚刚对每一列进行绝对值的统计
对于较大的我们认为是显著的激活值
那么请问如何定义这个显著
就是有这么多树摆在你面前
我们到底设置什么样的阈值来定义这个显著
以及我们如何去实现一个通道技术上
混合精度的这么一个扩大的kernel
就是说我们需要实现这么一个课程啊
就是说他在计算的时候
你要同时支持啊
有的元素是LP16的
有的元素是int4的啊
这是一种非常硬件不友好的行为
也是对开发者非常不好的行为啊
如果大家嗯有比较厉害的怪酷的大佬啊
可以去尝试一下能不能实现这个kernel
那在这种方法不太好实现的前提下啊
哦这个作者就提出了一种变通的方式叫SCALLING
关于这个SCALLING的方式啊
在实践上是非常巧妙的
包括我在第一次去看这篇文章的时候
我也感觉哇这个思路太牛逼了啊
不像是正常人可以想出来一个思路啊
且听我娓娓道来
我们现在考虑这么一个过程啊
就是我们在量化过程中啊
考虑权重矩阵W我们有个线性运算
可以写作Y等于WX
当我们对这个权重矩阵进行量化之后
可以写作Y等于经过量化之后的
这个W去乘以X关于这个量化函数啊
我们可以定义如下
首先这个德尔塔
这个德尔塔等于这个权重矩阵绝对值的最大值
去除以这个二的N减一次方
什么意思
比如说我们现在在做这个四比特量化
那么四比特的这个二进制数
它所能表示的最大值啊
就是二的4-1次方啊
这个德尔塔就是量化单位
那我们在这个量化的过程中啊
本来这个W啊
就是一个LP16
这么一个浮点数的这么一个张量
量化过程中
我们先去除以这个量化单位
然后进行一个取整
我们就把原来这个浮点数的矩阵
量化为了一个int4整数的这么一个矩阵
然后我们就可以把它存下来啊
就包括我们在进行运算的时候
就可以减少这个14的这个IO开销
那么之前我们也说了
我们在实际运算的时候并不是用int4计算
而是还是要把它转换成IOP16计算
那也就是说我们还需要一个反量化的过程
反量化的时候再去把这个量化单位给乘上
那么综合来看
量化的时候去除以量化单位
反量化的时候再去乘上这个量化单位
总的来说过程上还是一样的
但是由于量化过程有一个取证操作
它会带来一定的这个精度损失
那么这时候作者就说了啊
如果我们在量化的时候
对于显著权重进行一定程度的放大
它就可以去降低我们这个相对的量化误差
凭什么啊
作者是这么证明的
好好
马上我们就要经历一个非常非常非常巧妙的一
个思路过程啊
我们先来一步一步看
就是说对于权重当中的单个元素W啊
就可以认为这就是啊
原本我们这个矩阵很多数啊
很多元素对吧
我们把当中单个元素拿出来
就可以认为他是那个
0.1%到1%的形式权重
然后引入一个缩放因子
引入缩放因子之后
我们在量化的时候
对于形容权重把这个S给乘上乘上之后啊
再去除以这个新的量化单位
这个新的量化单位跟原来量化单位
理论上应该是有点区别的对吧
因为毕竟原来是拿这个整个权重矩阵的
这个单独的W它的最大值去除以二的N1次方
现在呢我们要先乘上一个X然后除以量化单位
那么我们新的这个量化单位
肯定也是去拿这个WS的最大值
去除以二的N1次方
这是这个量化过程
然后再反量化过程
我们再去乘上这个X分之一
也就是说综合来看
我们在量化的时候先去除
先去乘以这个X
反量化的时候再乘上这个S的倒数
所以说综合来看
这个效果上跟这个公式一
在计算过程上还是等价的
但是作者就说了啊
如果我们在量化的时候乘上这么一个缩放因子
它是可以降低我们这个相对的量化误差的啊
为什么他们的误差
可以分别用以下两个算式来进行表示
可以看到啊
我们在计算的时候
权重去除
以这个量化单位
以及后期反量过程中去乘以这个量化单位
他们都是在这个LP16的这个空间下进行
进行运算的
他们是没有任何损失的
唯一的损失是来源于这个取整函数
关于这个新的量化单位
还有之前的量化单位啊
作者提了这么一点
我们现在这个W啊
他是一个非常少量的显著权重对吧
他是以前这个权重当中
以前这个权重矩阵有非常多的元素
现在我们这个显著权重只是众多元素当中
非常少的那一个非常少的这一个数
哪怕是乘上这么一个缩放因子
它也很大概率不如之前权重当中的最大值
大对吧
这是个概率问题
因为我们这个W很少
它只占0.1%到1%
所以把它拿出来之后
它很大概率乘上S之后
还是不如之前的这个最大值大
所以说很大概率上
即使我们对这个显著权重乘以了S
但是不影响总体上的这个量化单位
这个量化单位
次德尔塔跟之前的德尔塔大概率还是相等的
那么对于这个round error啊
因为我们这个进行取整之后
这个误差近似在零到0.5这个区间上
均匀分布
它的平均期望是0.25左右
我们把它和它进行一个相除
它们的比值就可以表示为
次德尔塔除以德尔塔乘以X分之一
前面这一项因为它们大概率相等
所以可以暂时忽略
那么现在我们这个S因为是大于一的
S越大
我们这个相对误差就可以越小
当然啊这个S也不能太大
因为S如果太大的话
我们的显著权重需乘以S
它就有概率去超过这个权重矩阵的最大值了啊
从而影响我们这个成立条件
你刚刚我们这个其实这个推导过程
其实引入了一个非常关键的假设
就是关于这个量化单位
那我们刚刚这个假设是否成立呢
我们可以通过实验来证明这件事
作者做了这么一个实验啊
他在这个OOBT6.7B这个模型上设置
我们刚刚所提到的那个缩放系数啊
在S等于一
S等于1.25
1.524
那么一系列情况下啊
对我们这个显著权重进行一个缩放
然后来计算我们这个缩放前后
这个量化单位的这个这个
这个相对的一个情况啊
对于S等于一来说
就是没有对显著权重进行缩放
那么显然呃
前后这个次德尔塔跟这个德尔塔
它一定是相等的
所以说这个不不相等的比例是0%啊
这一列我们就作为这个BCLINEBCLINE啊
先不看了
当这个S等于1.25的时候
我们发现这两者不相等的概率
仅有2.8%
也就是说
他们有97%以上的概率是相等的
这也就证明了之前作者做出的那个假设
是有一定道理的
所以作者就提出了这么一种新的量化方式啊
就是SCALLING的量化方式
我们对所有的权重均进行低比特量化
但是我们对于显著权重
就是少量的显著权重乘以较大的S
等效于降低它的量化误差
对于非显著的权重
我们乘以较小的S等效于给予更少的关注
那么在实际操作的时候是如何实现的呢
好现在
这个就是我们之前所说的那个激活值X啊
对于X的每一列
我们去求这个绝对值的平均值
也就是得到了这个SX
对于这个xx乘以一个缩放系数之后
乘以一个缩放系数之后
就得到了每一列的缩放系数
得到了每一列的缩放系数之后
在量化阶段啊
我们就分别对权重的每一行去乘上这个S
从而进行一个量化
那现在我们就来结合这个IMDEPAY
在量化过程中过程中的这个命令行
我们来解释一下
每一个指令分别是来做什么的啊
MMD配在量化的时候
它的指令是啊
Md ply light
说明我们要进行量化
然后使用这个o to a w q的
这么种这么一种算法啊
啊
这个参数是指定我们用来这个标定的数据集啊
因为因为我们不是说要去计算这个激活值
每一列的这个平均大小吗
在计算这个平均大小过程中
我们是需要一个变动数据集的啊
然后这个数据集当中
我们的样本数量取一取128啊
然后每一条样本的这个序列长度啊
就是呃就是这一列的长度啊
我们可以指定这个2048
然后量化的这个位数呢是四位
然后分组通道数
分组通道数就是指的啊
我们我们以多少个为一组取计算值X
因为举个例子啊
比如说我们这个
比如说我们这个embedding这个向量
这个hidden size是2048
然后我们就可以128为一组
然后再128为一组
再128为一组
一百二十八一组为单位来去统计这个S啊
然后这个batch size不用说了啊
这个然后这个关于这个search score啊
就是关于这个阿尔法是否需要去进行一个优化
搜索啊
这就是整个我们这个AWQ算法的一个实现的
一个思路
下面我们来介绍第四章大模型外推技术
首先介绍到底什么叫外推呢
这个外推其实就是一个训练和预测阶段
长度不一致的这么一个问题
因为我们在训练的时候
通常我们这个数据集的长度
一般我们就会设置成比如说4096啊
8098
192或者其他的一些数值
但是我们在预测的时候
我们可能需要模型外推到呃
比如说16K文本
32K文本
128K或者是甚至一兆的长文
这会导致我们这个模型在训练阶段和预测阶段
它所接接触到的这个序列的长度它是不一致的
那么这个外推它会引发两大问题
第一个问题就是
我们预测阶段可能会让模型用到了
没有训练过的位置编码
因为毕竟啊深度学习他在这个拟合数据的时候
这个模型不可避免的
在一定程度上会对这个位置编码过拟合
那我们突然引入一个新的位置编码
对模型的这个泛化的效果肯定是有一定影响的
第二点也是比较重要的一点
就是我们在预测的时候
注意力机制所处理的这个token数量
是远超训练时的这个图文的数量
它会引发一个什么问题呢
就是导致这个注意力计算的时候
这个商的差异比较大
那么今天我们主要是是从这个位置编码的角度
来考虑
如何解决这个长度外推性的一个问题
然后关于第二点
大家可以上网上去搜集相关的资料去了解啊
从位置编码的角度
我们首先来重新思考一个问题
就是大模型为什么需要位置编码
在transformer之前
大家处理序列问题的时候
都是用循环神经网络
循环神经网络最大的一个问题是
就在输入序列时候需要逐个输入
不能并行输入
这个自注意力机制虽然解决了并行输入的问题
也导致了一个新的问题
就是理论上输入的多个嵌入向量
对于注意力机制来说
它们是等价的
也就是说
注意力机制并不具备区分token相对位置的能力
所以说我们就需要给每一个embedding向量
再增加一个位置编码
从而让我们的这个注意力机制啊
能够识别这个embedding向量一个相对位置关系
那么如何来设计这个位置编码呢
第一个思路啊
我们是不是可以直接使用一个整数
作为位置信息提供给模型呢
啊比如说我们有一个4096的序列啊
第一个token
我们可以给这个embedding向量增加一个维度嘛
啊然后这个维度啊第一个就是给一
第二个给二
第三个给三
第四个给四
然后一直第四先给4000
这样可不可以
好像是可以的
但是数值跨度过大
考虑我们实际这个模拟训练的过程啊
这个数值跨度太大
他对这个梯度的优化器它是不友好的
它会导致这个模型学习困难
所以这并不是一个比较好的方案
那么针对这个问题
我们可不可以把这个数值缩放到零一区间
提供给模型呢
比如说1~4000
我们线性缩放到零一区间
那么一的话就是0.0002
五二的话是0.00050啊
以此类推
一直到4000等于1.0000
可不可以
好像也是可以的
但是由于数值跨度又太小了
模型和优化期都不易分辨
所以说这个深度学习模型它非常矫情啊
就是说你这个数值的跨度不能太大
也不能太小
那怎么办呢
那么现在我们考虑使用一组向量来表示位置
什么意思呢
我们以这个十进制为例
位置1234
我们就用一个四维向量1234来进行表示
那么一般的对于位置N
我们就可以用如下向量表示
那么比如说从最低位开始
它其实就等于我们这个位置N去对十的零次幂
除法之后进行下取整
再对十取余
对于十位就是原来的这个位置
N对十的一次方整除对十取余
对于百位就是拿位置N去除以十的二次幂
去对十取余
然后往上以此类推
那么更一般的
如果说我们不是用十进制去表示
而是用贝塔进制
第I位数就等于位置N除以贝塔的次幂
再对贝塔取余
那么现在我们再来重新看一下
Attention is all you need
也就是transformer最原始的那篇文章
它的位置编码是如何做的啊
它的位置源码是这么一种形式
乍一看非常的让人困惑啊
就为什么要这么设计
那么如果说我们现在进行一个换元啊
我们用这个贝塔去代替原来的这个SA的这个
D分之二这个数啊
那么现在上面的这个位置编码
就会等效为下文的这种形式
那如果现在我们再来看一下
刚刚我们推出的关于贝塔进制编码的理论啊
首先可以看到里面的这个N去除以贝塔的次幂
这个是一样的
然后这里啊我们是做了一个整除再取余的操作
重点在取余上
这个取余的主要特性是想利用它的周期性
使得我们这个求出来的数值啊
分布在0~10之间啊
这个推广到贝塔进制
就是让它等让它在这个零到贝塔之间啊
不能大于等于贝塔对吧
那么这个周期性它其实跟这个sin cos
这个周期函数它是有一定等效性的
那么从这个观点来看
这个transformer使用的这个位置编码
其实就是一种特殊的贝塔进制编码
那么现在我们再来看这个外销的情况啊
还是以这个十进制为例
假设我们在训练的时候只训练了1K的余量
也就是说我们的位置编码
只分布在0~3个九之间啊
最长是三位
现在我们要求这个模型要外推到2K
需要四维瑞码
这会导致一个什么情况呢
就是说训练的时候我们的位置编码只有三位啊
那么我们在训练的时候可能会出现000啊
啊0012002003
一直到这个999都有可能
但是我们在推理的时候
突然之间因为超出了1K
我们就需要引入第四位
但是我们的位置编码只有三位数呀
这个怎么办
这个这个肯定是不能兼容的
那么这里就想到了一个办法
就是说我们的方案时候
我们在训练的时候就预留好足够的位数
就比如说虽然说我们训练的时候
我们的语料只需要三位的位置
但是我们也提前预留好一位啊
这样的话我们在外推的时候就可以自动的
如果说啊那个最高位不够了
我们就可以进位啊
这个第四位我们就可以利用起来
那么其实我们这个transformer
原作者就是这么想的
他认为预留好足够位数之后啊
利用这个正弦和余弦的交替啊
这种特殊的贝塔位置编码
就可以让模型具备关于这个位置编码泛火性
但是现实情况是啊
模型并没有按照我们的期望进行泛化
经过实验发现
模型至多能外推20%到30%
就已经相当不错了
超过了限速之后
模型的效果会断崖下跌
大家可以试试啊
这个模型的输出效果
就基本上就是一种乱码的状态啊
还有一种还有一种原因就是我们在训练的时候
虽然保留了高位
但是高位始终是零
这就会导致这一部分的这个位置编码
这个位数没有被充分的训练
使得模型无法处理这些新的位置编码
那么业界就提出了一些新的方案
比如说线性内插法
这个方案的大致原理
就是把这个新的长度范围等比例
缩放到我们训练阶段使用的长度范围
比如说啊
假如说我们适应的时候使用的是1K的语料
现在需要外推到4K
那我们就将0~4K这个范围
线性缩放至now1K
这会导致一个什么样的效果呢
就说我们训练阶段啊
最长只需要用三位
现在需要维修到四位了
怎么办呢
我们这个最高位没有啊
不能兼容啊
那我们就用内插法吧
我们就把原来的vb编码整体去除
以四就是等比例缩放为原来的1/4
那么现在我们的这个呃
呃需要呃这个预测的位置编码
这个2345÷4之后
就等于五五百八十六.25啊
就是通过线性内插等效为了三位
这种方法会导致最低位非常的拥挤
通常需要对模型进行微调
才能使模型适应这种拥挤的映射关系
并且各维度的这个步长它的差异是很大的啊
就除了最低位
最低纬这个非常拥挤
他这个步长可能0.0几
0.00几
那其他的位置的差异基本都跨度都是一
那就是89812345678
就是跨度是一只
有个位的差异比较小
这会导致模型不易分辨啊
效果还是不好的
那么这时候我们就有一种全新的方案
就是进制转换大模型
其实并不知道我们输入的位置编码具体是多少
进制的
我们现在知道啊
我们这个位置编码是一种特殊的贝塔
进制编码啊
其实实际在使用的时候
贝塔通常等于1万模型
知道这个贝塔是等于1万吗
他其实不知道
他只知道他所接受的这些数字的一个相对大小
他只对这个相对大小的关系敏感
比如说他肯定能知道这个这个三比二大对吧
呃十比九大啊
11比十大啊
这种相对位置关系他是知道的
那我们能否通过这种进制转换来等效内差呢
就比如说十进制下
三位的表示范围是0~999
但是我们在实际使用的时候
我们用16进制来表示
那么现在同样是三位数
16进制下
它的表示范围就是三个F啊
这个三个F在这个16进制下
它其实那个等效于十进制的4095
那什么意思呢
就是训练阶段我们是拿十进制训练的啊
就是等于233
这是训练阶段
然后外推的时候
我们需要外推到二两千三百五十了
2350乘不下
但我们用了一个非常巧妙的方法
我们呃不用十进制了
我们用16进制表示
用16进制表示之后就等于九二十四啊
转化为收入进制之后仍然为三位
但是模型他知道他知道14比13大
13比12大
这种相对相对关系他也知道
所以说哪怕我们由十进制转换到16进制模型
也是有一定的这种泛化能力的啊
就是这个嘛
就是虽然说每一位上都有可能出现大于九的数
但是每一位上这个步长
这个相对大小差异仍然是一模型
依然是有能力进行泛化的啊
这样呢我们就把这个内插的压力
平均的分摊到每一位上啊
这个技术其实就是呃大家应该听过了
其实叫n DK aware beta技术啊
就是我们在预测阶段
对位置面板的base进行一个缩放
也就是说对于新的位置N对于内插法
我们就是想用原来的这个base
然后把我们这个原来的位置呃
等效变为原来的K分之一
从而使得适应
现在呢我们不用这种内插的方式了
我们想要通过这个N它还是N
我们只是对这个贝斯进行一个拉姆达的放大啊
放大之后让他能去等效这个内差
这个N呢就是实际头的长度
K呢是实际长度与训练长度的一个比值
经过求解求解之后
我发现拉姆达等于K的D减22次幂
然后把这个拉姆达再换回原来的位置
编码的形式之后
我们求得我们只需要对原来的这个base去乘以
这个K的D减二分之D次幂
就可以实现这个NTKV2这么一种Y的方式
那么现在我们来看IMDEPLAY具体是怎么做的
实际上完全是对应着我们这个NDK52
这个理论来的啊
首先呢这里max sequence length
就是当前我们这个序列的实际长度
Max postin bin
这个是训练的时候所使用的最长的序列长度
然后这里呢
其实你可以认为它就是计算了这个比例K
只不过呢他又加入了一个SCALLING系数啊
这个是指定的
一般我们在MD8的时候会指定2.5啊
但是这个本质上就是去计算这个K
然后他去计算这个新的这个旋转位置
编码的这个base就是这个theta原来的theta去乘上
原来这个谁带去乘上
可以看到就是这个K的D减二分之D次幂啊
就得到了这个新的旋转位置编码这个theta好
接下来我们来看最后一张function calling
那么什么是function calling呢
又为什么需要有这种function calling的机制
简单来讲
方式
calling就是让大语言模型调用外部工具
来解决语言模型原本解决不了的问题
从而来拓展大语言模型的能力边界
function calling可以用来解决时效性的问题
比如说回答今天是哪一天啊
今天气如何呀
这样的问题
因为大圆模型的训练数据截止在过去
所以大V模型并不知道最新发生的事情
另一方面
方式calling可以拓展大语言模型的能力
可以让他去解决一些比较复杂的数学问题
如果在Python解释器的帮助下
这样的数学计算情况变得非常简单
而且精度比较高
接下来我们用一个例子来简单介绍一下
function cony的原理
假设大圆模型部署在云端
用户首先在本地向大圆模型提问
今天天今天天气如何
并传入大圆模型能使用的相关工具信息
云端的语言模型在接收到请求后
会输出它需要调用的函数以及需要传入的参数
随后这些信息会传回到用户的本地
用户在接收到函数调用信息后
在本地执行相关的函数并得到结果
然后用户将函数调用的结果
返回至云端的大语言模型
大A模型在接收到函数调用的结果后
会整合相关信息并进行下一步操作
最终输出回复或是下一次调用的函数
在这里
大运模型就输出了
最终回复为
查询到天气为4℃的这么一种信息
大圆模型再将最终的回复返回给用户
那么用户就心满意足的完成了本轮对话
那么大家如果觉得不错的话
也欢迎给MDEPY设7.1个4start
Hello
大家好
欢迎来到l m d play量化部署进阶实践啊
整个实践部分呢主要分为四个章节
首先第一个章节呢就是配置一些基本环境
那么在第二个章节以及第三个章节中呢
我们将会带领大家呢使用l m d play呢
呃进行一些这个API部署
那么除此之外呢
嗯我们还会带领大家探索一下the l n t play light
这样一个量化部署功能
在最后一个章节能带领大家体会一下子
这个如何进行一个API开发
把它封装成一个独立的API呃
以及新出的这个function call功能
那么废话不多说
直接开始实践环节
首先呢我们需要打开inter studio环节
创建一个开发机
OK啊点击创建开发机
个人与团队无所谓
任选即可嗯
开发机名称呢可以自拟一个
我们这里呢就选用l m d play呢作为开发机名称
由于呃环境要求呢
需要我们使用这个COA12.2镜像
所以请选择COA12.2
OK我们就来到了这个资源配置环节
诶这么多配置选项
我们选哪一个呢
我们首先就需要回归到我们所要选用的这个
模型参量本身来进行探讨
首先呢我们要运行的是7D的一个英特尔
L m 22.5
那么由这个马超呢我们就可以进行一个查询诶
这是他的hui face页面
我们可以清楚的看到呢
他这边的这个呃设精度呢
被设定设定成为这个b float16
也就是呃16位的这个浮点数格式诶
然后我们就可以带领大家来计算一下子
对于一个70亿参数的模型
那它每个参数使用16位浮点数诶
16个浮点数需要两个字节进行存储
那么单它权重的大小呢就会需要14GB的显存
也就是说我们需要一个大于14GB显存的
一个机器诶
我们再切回到这个呃资源配置页面
我们就可以看到是我们选用这个30%
A100就可以满足运行需求了
那么在最后呢
运行时长呢可以进行个人的这个合理设置啊
我们就默认是八吧
等待这个状态从排队中呃
变为这个使用中
我们就可以运行中
我们就可以进行这个进入开发机了
那么首先映入眼帘的呢
就是我们这个开发机的界面了
那我们回到这边来
哎
我们需要开始创建一个l n t play的康达环境
我们复制第一条指令啊
来进行一个输入
这条指令是什么意思呢
这个是create创造一个新的环境
它的名字呢叫做lm d play
我们设定呢呃Python版本呢为3.10
然后杠Y呢就是确定创建环境
这样子我们后续呢就不需要再进行一个呃
确认了
需要经过一段时间等待啊
这段时间大家可以自行的
这个干一些其他的活儿
OK啊经过一段时间的等待呢
我们配置完成了
那么下面呢就会进行一些最基本的这个
环境安装
那么首先呢我们需要这个active l m d play
激活环境嗯
等从base呢切换到lm d play呢
就算我们进入到我们配置好的这个呃
不是配置好的建立的虚拟环境中了
那么现在呢
我们需要进行一些这个最基本的环境配置啊
直接CV就可以了
那这也是需要一段时间的
大家静心等待哦
经过漫长的等待啊
我们终于是把虚拟环境装装好了
那么此后呢我们还需要装一个三个包啊
这个是最后的一个额外的补充
OK啊经过一段漫长时间等待
我们终于把环境配置好了
那么下面呢就需要嗯创建一个存放模型的目录
为了方便管理嘛
那么首先呢我们复制第一条命链接呃
第一条命令创建一个model文件夹
创建完文件夹之后呢
我们就可以在左侧的这边呃
管理系统看到我们刚刚创建好的models文件夹
然后呢输入两条呃命令
创建一个软链接
这样子呢我们就可以直达到我们的这个模型
输入完之后呢
我们可以在左侧这边查询
分别呢这个英特尔LM22.57b chat
以及英特VL二二十六B
这两个模型呢
已经存放到我们的这个呃models目录下了
那下面就会来到我们这个呃
基础环节的最后一个部分就是在量化开始前呢
我们需要先验证一下环境呢
呃是否能够正常工作
模型文件呢是否是完整无好啊
完整无损的
那我们直接就是嗯启动一个chat对话进行即可
然后我们就可以看到这个模型已经启动完成
当它出现这个double end to end input的时候
就代表已经可以与模型对话了
嗯值得注意的是
需要两下回车才能这个额进行输入
那么此时呢
我们就可以在命令行界面呢
与这个模型进行一个对话
这边是一个对话的一个示意图
大家可以自行探索
那么这个时候我们需要注意一下子
就是右上角的这个资源监视器啊
显存显示占用约二十二十三G23GB诶
这是一个先先圈起来啊
之后会用得上的一个小知识点
那么如果说呢
呃有同学选用的是50%
A100建立的机器呢
那么同样运行这个一样的模型
就会发现此时的显存占用呢是36GB
那么这是为什么呢
唉这就是我们需要不得不提到的
这个l a l m d play设置的一个kv catch的问题
在呃之前的我们谈嗯已经了解到呢
这个LM2.57B的模型呢
它的精度是BF16
那么它呢权重文件呢
需要占用这个14GB的显存
那么同时呢l m d play的默认设置下
它是占用这个kv catch是0.8
也就是意味着呃占用剩余显存的80%
所以说呢这个时候我们就可以来计算一下子
而对于30%的A100
也就是24GB的显卡
我们权重占用了14GB
那么剩余显存的就是十个G
那么十个GB乘0.8就是8GB
那么总共显存的就会占用22GB
还有一些系统性的呃
其他项目项目的一些这个占用
所以说呢显示是22.9
也就是约23GB
那么对于40G的显卡呢
也就是50%的A100
那权重占用同样是14%
那么拿40减去这个14等于26
26×1个0.8
就是20.8GB那么加上原来的14呢
就是占用34.8GB显存
那么同样的还会有一些其他项目的占用嘛
所以说呢因此呢这个实际占用会略高于这两个
我们算出来的数值
那么除此以外呢
如果说我们想要自己的实现一些这个呃
显存占用的监视情况呢
我们可以新开一个终端
然后呢输入这个嗯嗯NVIDIA杠SMI
或者说studio杠SMSMI命令进行一个呃监测
这个时候呢它会显示是22.9
也就是和和我们显存占用一样的
验证完模型以及这个启动器之后
那我们就会进入到第二个章节
lm play与英特尔LM2.5
那么之前呢我们启动呢是直接在本地部署的
我们直接就是chat
那么在实际应用中呢
我们会把它这个封装成一个API接口
然后更方便的进行一些对话与访问
那么来到于此呢
我们先exit退出一下
OK退出完之后呢
我们清理一下这个UI界面吧
首先呢
第一步自然还是需要先激活这个
我们的l m d play环境啊
我们这里已经激活了
就不再进行继续了
然后我们直接输入以下命令
开启这个API服务器
在启动过程中呢给大家介绍一下子呃
首先呢LIDEPAY
serve是启动API服务器的这个呃命令
然后呢第二行呢root models
英特LM22.57b chat
这是一个模型的路径
然后呢呃model fat呃
model format就是模型的格式
然后呢qun policy量化量化的一个策略
然后呢剩下两个呢
就是呃指定一下子这个网络接口和端口
以及最后一个声明一下子使用的这个GPU数量
那我们稍待他这个启动片刻
OK那我们等待它启动成功之后呢
就是如如下如这个图所示的一个UI界面
诶此时我们点击这个链接呢
诶是会发现是blank
就是无法访问的
为什么呢
因为此时呢
因为此时呢我们是在虚拟机上进行操作
如果说我们想在本地进行操作的话
我们需要进行一个嗯SS转发
那么此时呢我们需要是打开一个CMD
或者说是power shell接口
那CMD和power shower都可以直接访问到我们这里
先拿CMD呃作为演示吧
我们复制以下指令
诶
这个时候你的端口SSH端口号怎么查询呢
在创建开发机这边有个SH连接在这里啊
可以自行查找自己的这个端口号
每个人端口号都不一样
请自行查找
这里能否的是呃
49300啊
回车
在第一次输入的时候呢
他会要求你这个呃向你确认一下是否继续连接
这时候呢我们需要手动敲入yes
然后回车即可
而现在呢会要求我们输入这个密码
直接是复制
然后这边CTRLV一下
然后回车即可
那么同时呢值得注意的是
就是在呃命令行窗口中默认的这个密码呢
它是不显示的
也就是说诶我们这边呢是不会显示出密码的
这个铭文的
那么我们直接回车即可啊
让窗口就保持在这么一个状态
哎我们就不用管了
然后我们切回到这个界面
此时我们再点击这个UI接口
我们就可以看到fast API的一整个这个状态了
然后我们我们可以看出来它有一些呃
有一些这个操作窗口
以及一些这个呃用途
用途呢其实很简单
我来看一下
首先呢我们点开这个第一个get
然后呢我们这边需要输入它这边这个id
也就是说把这个control c一下
然后把这边models Ctrl v一下
然后count这边可以自行修改
就比如说
给我讲一个小故事
然后点击execute
稍待片刻咳
我们就可以看出他的回复是呃
嗯他的回复就是这个样子
然后此时呢我们切回我们的这个后端
你们就会发现这个相比于之前的嗯
直接显示网口
我们多了三个get以及一个post
这就是一个后台的接收消息
以这个post传递消息的一个过程
然后200呢是表示状态嘛
就是OK就是传递一切正常的一个过程
那么此时呢
就代表着我们成功开启了这个fast API服务
那么呃我们现在可以以多种方式
与这个API服务器呢进行一个连接
那么首先呢让我们可以
我们可以关闭掉这个服务器
然后呢让我们新开一个这个终端
这终端接口
然后我们首先需要还是一样的
都需要激活这个呃命令环呃
虚拟环境
然后呢我们可以运行以下命令
就是访问这个23333端口
也就是我们刚才的呃
设置的这个API服务器的端口诶
这个时候它就会自动的进行一个对话
那我们稍待片刻啊
OK我们就进入到了这个对话界面
此时的对话呢乍一看呢和之前我们直接启动嗯
chat启动的话是相似相同的
但是实际上呢呃我们还是经过这个后台
与这个呃API服务器
进行一个发送和接收的一个功能
那此时如果说你想与大模型对话的话
用法和之前chat的用法是一模一样的
我们这里呢只做演示就不做过多深入了
我们只输入EXCET退出
然后下面呢我们需要以这个GRADU网页形式
来连接一个API服务器啊
那么此时呢我们就也是一样的
一键即可这个启动网页
启动好了之后呢
呃我们可以理论上可以点击这个URL访问
但是呢呃一样的还是无法访问
原因是我们本地没有做这个的SH单发呃转发
那此时呢我们关闭这个CMD界面
然后呢进行一个新的端口号的转发啊
这个时候呢我们用power shell进行一个演示啊
其实两边演示是一模一样的
同样呢是呃查询自己的这个端口号
比如说是我这边还是49300
然后输入输入你的password密码
然后第二次输入呢就不会让你确认了
直接就是呃跳转到这个界面
OK我们把它放在这个界面
然后点击这个链接就可以进行一个访问
此时呢他们的模型名称呢就是我们呃
我我们的这个路径名称
那我们可以进行一些参数的调整
以及一些这个进行对话
那么这边呢就你好好
OK这里就是一个正常的这个输入输出对话功能
那么呃只做演示
不做过多的深入和拓展
我们来到呃下一步重头戏l m d play li
嘿就是说哎我们现在启用一个大模型呢
我们需要占据23G的这个显存
这个显存是十分庞大的
那我们有没有办法把这个显存变得更小一点呢
那呃i m d play呢是提供了一种方式
就是说嗯可以把这个通过命令来
把这个kv catch
默认占用的这个剩余显存空间的比例呢呃减小
就比如说我们此时复制这条指令
来到这个UI界面
OK啊我们是全都退出
我们新建一个终端
然后进入到这个
OK我们先进入到
m l m play环境中
然后我们启动这个呃模型命令
然后呢同时注意呢
我们这时候多了一串小乙嘛
就是我们把这个max entry count设置为0.4了
诶好我们现在就成功启动了这个大模型
但是哎我们来观测一下
此时的我们只占用了19GB的显存
这是为什么呢
这个4GB少的是哪儿少了呢
我们可以来算一下子
对于之前的这个
我们占用的就是直接chat启动的
占用的23GB呢是74GB的权重
以及这个8GB的这个k v catch占用
以及其他的一些项目呢是1GB的显存占用
组成了14+8加一
等于23GB的这个总的显存占用
那我们修改之后呢是19GB
那么模型权重占用呢依旧是14GB
我们没有动
我们动的呢是kv catch占用哎
发生了什么呢
我们把kv catch的修改呢设设设置为了呃
占用40%
那我们此时呢就占用了呃
100.44GB的一个占用
那么我们相当于相当相对于23GB呢
我们少的这个4GB呢
就是呃减少了kv catch占用的这个呃显存空间
那我们呢还有另外一种这个量化方式呢
就是kv catch int4int八量化诶
我们可以这个输入以下指令呢
一键开启这个int4int8量化
我们这里呢就是以这个int4量化为例子
进行举例
同样先输出这个except
然后我们输入指令开启这个API服务
那么此时呢我们显存占用的是19呃
19gb
那么此时的用法呢
如果说还想与此时状态模型的这个
进行一个对话的话
我们可以回顾之前的这个以命令行形式对话
或者说以GRADU网页形式连接API服务器啊
步骤完全一样的
那么诶相比于之前的这个呃直接启动模型
显存占用呢23GB呢
我们会是减少了4GB的一个占用
但是有什么区别呢对吧
就像我们之前直接设置这个呃
kv catch缓存为0.4
我们这时候多了一句这个呃QU的policy4诶
我们好像显存占用并没有更少啊
呃因为呢此时状态下a LLM d play
开辟的这个kv catch空间呢
呃所储存的这个向量精度不太一样
哎qut policy设置为四的时候呢
我们是使用int4进度进行量化的
呃如果说默认是count policy0的话
我们将将会使用这个bf16精度
那么两个精度一比较
我们可以鲜明的得知
bf16精度占据这个呃两个字节位
而呃int4呢只占据这个0.5个字节位
那么所以呢我们可以相同的这个空间
存储更多的这个元素数量
这样能带来了更大的一个数据密度
那么下面呢就会来到一个重头戏之一啊
W4A16模型的量化与部署
W4A16呢
在之前的PPT呢已经有了一个呃详细的介绍
我们这里能总结一下子
就是那意味着权重量化为四位整数
但是呢那个激活activation
依旧保持16位浮点数
那么呃再回到这个模型算法啊
回到我们虚拟机这边
我们输入以下指令啊
直接一键开启这个呃四比特权重量化
那经过我们漫长的打包环节啊
我们可以看到此刻呢嗯我们推理已经完成了
在我们的这个models文件夹下
可以直接看到我们刚刚设置的这个推理文件夹
W4A十六四比特已经推理完成
那么下面我们就要开始
进入到我们的这个比较环节了
就是它完成推理以后
和原来的完
和原来的这个模型文件有什么区别呢
那么首先呢呃最直观之一的表现呢
就是模型的文件的大小
那我们可以看到呢呃输入输入命令以后
我们进行这个呃对文件夹大小体积的一个观测
我们推理后完成的这个呃
模型文件呢是4.9G
那么对于我们原先的这个
原先的模型大小是多少呢
我们来进行一个比较
那因为之前的这个
我们是以软链接的形式进行一个链接的
所以说在这个当前文件夹内
它是不占据这个体积空间的
那我们现在呢来到了我们的这个目标文件夹
来查看一下子INTELM2.57b chat
它是原本的大小是15G
那么经过推理后呢
它被优化到了4.9G
这个体积方面是十分明显的
那么与此相关的呢
更明显的是他的这个显存调用
那么我们下面就进入到
这个显存占用的一个比较
那么同样的输入命令开始一个模型的启动
那我们是直接进行一个chat
OK他们现在启动完成
启动完成之后呢
我们观测右上角的这个显存占用情况
此时占用的是20.998
那么诶我们和之前的23G的
这个显存比较呢
是少了2GB的这个显存占用
那么这2G币又是从何而来的呢
一切都是呃是有因因有果的
那么我们先来呃
对这个23GB的这个情况进行剖析
那么值得注意的是
此时不同的是呃bf16精度下7G7
7B模型全程占用14GB
那么对于我们推理量化之后的呢
能在int4模型精度像7B模型的这个权重呢
它仅仅占用3.5GB的这个显存空间
为什么呢
因为那个BI呃
b float16是16位的这个浮点浮点数格式
它的16位呢就会占据两个字节的存存储空间
而int4呢是四位数四位的这个整数格式
它只占用0.5
所以说呢对于原本的14GB
我们除以个四
得出来3.5GB显存空间占用的这么一个参数
那么此时呢相应的剩余显存呢也会发生变化
24-3.5呢就会变成了20.5
然后默认占用80%
也就是呃16.4GB
那么其他项呢照例是1GB的占用
那么总共呢我们就可以算出来3.5+10
6.4+1等于嗯
20.9GB的这么一个显存占用
那么诶有小伙伴就想问了
说说了那么多
哎我们说了这个推理量化
我们说了kv catch set
我们还说了这个kv kv catch的int4量化
有没有可以说我把它都用上了
当然是可以的
那么最好的方法就是诶我们全都要
那么我们直接输入以下指令就可以进行一个呃
把所有的这个指令进行启用
我们可以看到我们进行了这个kv catch
int4的量化
我们设置了这个kv catch的这个呃默认占用比
以及我们调用了这个量化后推理
量化后的这个W4A16的一个模型
那么复制命令到这里
首先呢退出之前的chat命令
然后我们调用这个模型
进入进入一个这个API服务器
OK跳出这个界面呢
就代表我们成功开启API服务器了
那么此时呢显存占用13.5GB
值得一提的是
就是这个状态下所有的通信呢都是一切正常的
就是说我们想调用
我们想以命令行形式
我们想以GRADU形式对这个呃
对这个API服务器进行访问都是可以的
按照之前交额之前教的操作如出一辙
一样操作就可以了
那么13.5GB呢就是一个呃
相对来说已经是一个十分友好的一个呃
显存占用了
那么呃我在这里呢也写了
这个13.5GB的这个情况是由何而来的
大家也可以自行进行计算
然后呃与我的这个呃结果呢进行一个比较
那么如果说呢想进行一个更极限的工况的话
呃可以自行探索
一般就是对这个catch max呃
count就是k v catch占比进行一个修改
那么也可以探索说哎
如果说我把它修改为0.001呃
几乎约等于禁用这个kv catch占用
那那那那时候与我与模型对话
会发生什么故事呢
那么各位小伙伴也可以进行自行探索
那现在让我们进入第三个章节
也就是l a m display与INTERVL2
那其实选用这个英INTERVL二二十六B呢
嗯本质上来说作为一款这个语言模型
也就是vm vision language model呃
在操作本质上面和之前所呃使用的INTELM2.5
在操作上没有什么区别
它呢仅仅是多了图片输入这个额外的步骤
但是额为什么还要多出这个环节呢
他的这个我们选用它的目的呢
主要是希望带大家体验一下子
就是量化部署本身的意义
就是呃让它从一个呃比较高的这个
机器配置要求降到一个比较低的机器配置要求
那么首先呢我们就会来到这个呃模型
量化部署嗯
那么切换到我们这个虚拟机界面
那首先呢还是老样子
我们先切换我们的虚拟环境
是l m play
那么下面呢我们就开始针对这个英特VR2
进行一个模型量化
直接复制
然后
OK在执行期间呢
我们可以先说一下
为什么我们不直接运行跑这个26B呢
因为嗯它本身跑完全版的话
需要约70加GB的显存
也就是呃一张这个完整的A180GB的显卡
才能跑的
但是现在呢我们是呃30%的100
也就是二十四二十四GB显存的这么一个环境
那么经过一系列量化操作呢
我们就可以呃实现在一张这个24GB显存的
呃机器上面
就可以跑这个26B的一个模型啊
这也是希望带大家体验的一个量化的魔力所在
那么让我们稍等片刻啊
等他进入这个进入到这个量化推理阶段
那等待推理完成之后呢
我们就可以在左侧的这个文件夹里头
看到我们所设定好的这个呃四比特文件
四比特模型向量文件
那此刻呢我们就可以输入以下指令
然后让我们启用量化后的模型
来观测一下显存的占用情况
那么启动完成之后呢
我们就可以观测这个显存占用情况
那么此时呢显存占用的是23GB
23GB又是怎么来的呢
那么根据英特VR2的介绍呢
INTERVR二二十六B呢是由三个部分组成的
那么对应的显存计算呢
我也放在了这个呃计算细节这个折叠栏中
如果说想自行呃进行计算比对印证的话
也可以进行一个参考
那么此时呢如果说进行推理图片的话
会提示这个剩余显存不足
哎因为这个我们在这个推理图片的时候
拍touch呢会额占用额外的一些显存
那我们此时的显存呢已经到达了这个极限
所以说呢呃如果说想进行这个图片推理的话
呃可以开启这个50%的A100
进行图片推理
那么主要的目的呢就是带领大家探索一下子
这个呃API服务器
那么此时呢
其实这个命令呢和我们上面这个开启的命令呢
是观测进行观测
显存的命令呢是一致的
那么剩下呢呃如果说想以命令行形式
还是GRADU网页形式进行连接的话
都是与这个呃第二章节中的内容完全一致的
那么整个章节呢就是想带领大家看一下子说呃
在24GB显存显存在显卡上面
我们也可以跑这个26B的
这样这样一个参数量很大的一个模型
哎这就是量化的意义
也是量化的魅力所在
那我们现在来到了第四章节
l m d play与fast API啊
同时是与方程靠这两个功能
那么首先呢嗯API开发呢其实就是启动
刚才我们所启动的API服务器
只不过不同的是呢
这次是由我们自己嗯进行呃
编程语言代码的创建
然后与服务器进行一次通信
这样子呢我们就可以对这个呃
本质呢其实是利用这个API呃
发送信息与接收信息信息
这样子呢我们就可以进行这个呃更轻量级
更方便的一个开发
那么首先还是一样的
让我们先进入环境
然后呢输入以下指令呢
我们启动一个量化好之后的这个呃
int m27.52.57B模型
那在启动完成之后呢
让我们呃依据这个图示新建一个终端
在新建终端呢
我们需要额新建一个这个PY文件
就是输入touch点root杠英特尔M2PY
那么此时呢我们在左侧的这个呃文件栏呢
就可以看到我们新建的这个文件了
双击打开
把以下命令呢直接复制进这个PY文件嗯
同时记得这个填写完成之后呢
我们需要这个呃CTRL加S保存一下
Saving completed
那这里呢我们向这个大模型发送了两条消息
一条消息呢是以system这个角色进行发送的
是你是一个友好的小助手
负责解决问题
那么第二个呢是我们以UUSER
也就是用户的角色进行一个呃发送消息
也就是说帮我们讲述一个
关于狐狸和西瓜的小故事
那么此时呢保存完之后呢
我们需要先呃进入到这个环境
lm g play的环境
然后呢我们直接运行这个点PY文件就可以
Python route int arm2呃
2.5点PY
那稍待片刻呢
我们就可以看到他的这个大模型的答复了
OK我们看到大模型的答复
返还了一个这个呃
关于狐狸和西瓜的一个小故事
那么本质上呢它依然是呃
利用API向服务器发送一个请求
然后呢服务器呢post出来他的答复
那我们可以看到这里也是一个两两个有呃
两个200的一个状态码
就是代表这个收发消息正常
那下面呢
我们会来到一个更更有意思的一个功能
也就是function CORE
function call呢就是函数调用功能
什么意思呢
就是说嗯大模型在进行一些额外的这个关系
调用的时候
就比如说我想知道今天的天气
我想知道一些这个复杂数学公式的结果
那么这个时候他的知识库储备可能就不不够
它进行完成这些这个操作
它会产生一些幻觉
那此时呢我们就可以这个嗯
给大模型添加一个function call呃
我们可以在调用模型的时候呢
向大模型说明这个函数的作用
然后呢这样子大模型就能自己根据用户的输入
把它这个输入结果呢
呃用户的这个问题进行一个拆分呃
输入参数的
然后把它发送给这个函数
然后函数完成这个输出之后呢
他把这个函数的结果进行调用
调用作为回答用户问题的依据呢
进行一个呃最终的回答
那么首先呢呃我们依旧是需要先进入康纳环境
那我们这边是退出原先的这个服务器呃
清理一下子这个界面
那么首先呢我们是呃启动这个原生的2.5
英特尔M2.57B模型
这样一个服务器
然后呢
此时呢我们新创建一个这个呃
INTEM2.5function的这个pr文件
那我们在左侧这边呢呃输入以下的这个
复制以下的代码
然后同时记得啊
输入完成以后一定要CTRLS保存一下子
这个时候呢我们就可以来仔细观察一下代码
我们是向函数呢添加了呃
我们是向这个呃大模型呢添加了两个功能
一个功能呢是呃A加B
我们简单的是把它命名为ADD
一个是一个呢是乘法
也就是A乘B那么他们的功能呢都很简单
哎但是我们给需要给他这个大模型
并不了解这个函数是什么意思
这个时候呢我们就需要自行介入这个description
那么首先呃定义这个name
就是AD
然后我们进行一个描述
它就是呃计算这个两个数
两个数字的之和
然后呢我们需要一个输入一个int整形数的输入
然后对输入的描述呢是一个数字啊
同时呢B呢也是一个int整型数的输入
他的这个输入描述呢也是一个呃数字
那第二个function呢也是一样的
是乘法
然后呢描述是呃计算两个数字的这个之积咳
OK那我们就可以来尝试调用调用呃
给大模型提供这两个数字
然后我们来让大模型呢进行一个呃回答
会是怎么样一个结果
值得一提的的是呢
我们这此时的输入呢是一个呃
我们的message呢是这样子的一个
我们是让大模型计算3+5乘二这样一个结果
那其实呢这个命令行语句呢很简单
即使不调用这个工具
大模型呢也能自行完成
那此时呢我们是更多的是展示一个这个呃
function的一个简单流程啊
供大家演示
大家来看一看
说大模型到底是呃
怎么样来进行一个呃function CORE的一个流程的展示
那么首先呢我们可以看到是
我们打印了两次print呃
一次是在这里print response
第二次是在这里print response
然后呢我们分别打印出两次这个function out
function的这个计算结果
以及最后的一个function的最后计算结果而来
更清晰精
更清晰更清楚地展示我们这个function call的流程
那么让我们现在直接是呃运行这个Python文件
哎好了
我们此时可以看出来嗯
Python呢已经把这个结果输出给我们了
那么首先呢是第一次response
第一次response
它这边finish的原原因呢
解出来是需要进行工具的一个调用
那么呢是他的这个他的这个说法呢
是为了计算这个呃
3+5的积乘二的这个表达啊
我需要进行把这个三和五呃
需要进行第一次三呃加累加起来
然后呢我们需要对这个结果呢呃与二相乘啊
所以呢此时呢他就需要调用这个at的工具
at的工具对这个三三的呃
对A的定义呢就是数字三
对B的定义呢就是数字五
然后此时呢我们就可以看到他对这个呃
第一次的这个function呢
output呢就是835乘得八嘛
那么第二次第二次继续
就是需要调用这个呃乘法来进行解决问题
他说的是呃
我们需要呃这个3+5的这个结果呢是八
那现在呢我需要对这个结果能乘以二
所以说呢他把A定义成了八
B定义成了二
调用了这个名称为呃multiplied
就是MULMUR的这个函数
然后呢此时呢他就输出了这个16的这个结果
这就是一个很简单的这个方针靠的演示
那么也希望呢
如果说大家日后呢有各自的这个开发需求
比如说你可以加入这个呃获取天气的
获取这个搜索结果的这个function
也可以加入给大模型啊
同时呢注意需要给大模型加入更精确
更精准的这个this description描述
那么呃本次的教程呢就到此告一段落了啊
希望大家呃进行自行学习
感谢大家的观看