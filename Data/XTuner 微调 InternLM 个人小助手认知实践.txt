大家好
我是杰米
今天给大家带来的课程是s turner
微调个人小助手认知
首先我们要了解什么是微调
本课程中的微调指的是大语言模型
微调是指在预训练的大语言模型基础上
使用特定的任务数据进行进一步的训练模型
使其更好地适应和执行特定的任务
基于书生普语
也就是语言模型
书生万象
也就是具有图像模态的语言模型进行微调
可以做到以下的几个能力
第一个是特殊领域的自我认知
比如说小助手
第二个是指的LOM或者VLM
获得处理特殊任务的能力
比如让vim处理grounding refer等任务
第三个就是从基座模型到chat模型的转变
那么本节课通过微调我们可以得到什么呢
我们举个例子
你是一家公司的算法工程师
老板心血来潮说
要做一个自家的大模型来用作智能客服
需要当客户问起模型是谁的时候
会回复公司的名字
但是这个时候你就非常苦恼
公司没有很多的显卡
让自己从零开始训练
也没有充足的数据
这时候该怎么办呢
你看到了这节课
想到哦
要不在其他的大模型上做微调吧
那么这节课我们带来了几个条件
第一个是显卡有限
第二个是数据有限
这两个条件来做训练
大语言模型自我微调这个任务吧
我们可以看到这张图
左边的是微调前模型
不会说自己是谁的小助手
而右边已经会说杰米的小助手了
在LM的下游任务中
一般有两种微调范式
增量增量预训练微调和指令跟随微调
增量预训练微调
一般用于让基座模型学到一些新的知识
如某个垂类领域的常识
所需要的训练数据
可以是文章
书籍
代码等指令
跟随位调一般用于让模型学会对话模板
根据人类指令进行对话所需要的训练数据
一般是高质量的对话和问答数据
也许还需要添加特定的任务
Token
我们一般用对话数据
也就是呃对话模型
也就是带chat的
整个对话模型的开发过程就如下图所示
在基座模型上进行增量预训练
作为我们的知识补充
这里我们使用的是INTERNM作为基座
这样就构成了我们的垂类机组模型
然后再通过指令跟随微调
使模型具有对话能力
由于增量预训练使用的数据比较多
而且锤类数据大家的需求都不一样
本节课主要是介绍
如何微调出一个拥有自我认知的对话模型
介绍完微调范式
我们发现全量微调需要很大的显存
而我们的显卡有限
该怎么办呢
这里要介绍一下LAURA和QLAURA
首先我们需要知道LOM的主要参数
做模型的linear层中
这些参数会占用大量的显存
而LAURA的思路就是在原本的linear层旁边
新增一个支路
而这个支路包含着两个连续的LINUX层
我们把这个支路叫adapter
而这个adapter的参数通常比较小
通过训练这个adapter
我们最终达到优化模型的目的
而由于其参数量比较少
我们可以用较低的显存完成微调
从左往右分别是全量微调
Laura kla
全量
微调在整个base模型都参与训练并更新参数
并保存整个base model中的参数优化器状态
而采用LAURA的时候
base model只参与forward过程
只有adapter部分进行训练
也就算by word更新参数
采用kill laura则是对base model进行进一步的优化
需要量化为4bit模型
优化器状态中
CPU与GPU间OFFLOW与LAURA相同
base model只参与推理
而adapter的参数进行更新
只保留adapter中参数的优化器状态
我们在第一期的实战营中
涌现了许多基于微调的作品
大家可以在实战营群里进行讨论
说了这么多
时间紧
任务重
这么多的优化方法
有没有一个方便的框架让我们一网打尽呢
那么我们就要开始介绍一下本节课的重点
As turner
书生全链条开源开放体系中
微调工具EXTENER能够适配多种微调算法和偏好
对齐算法
覆盖覆盖各类应用
包括大家熟知的INTERNM
书生葡语
拉满sama mystery
同义千问等
也适配多种开源生态
包括加载hin face models scope模型等
以及它们的数据集
还有自动优化加速
开发者无需关注复杂的显存优化与计算加速
细节
支持千亿参数加百万上下文训练
甚至还适配多种硬件
覆盖了英伟达二零以上的所有显卡
最低只要8G的显存
就能对7B模型进行训练
我们的课程的下一期也会推出
在华为升腾910B上的微调和预训练
大家可以再留意一下后续的课程
而ests turner在dance和MOE模型训练方面
ests turner提供了极致的显存优化
在超大模型和超长序列的全量微调方面
S3呢相比起其他的竞品表现要更好
同时在MV模型的训练上提供了通信优化
ests turner还提供了零显存浪费的偏好
对其训练方案和并行训练的功能
说了这么多
老板等不及了
现在就要开工
首先我们要开始配置环境
现在要打开我们本次课程的x turner
训练的文档
然后我们根据文档进行环境配置
首先是打开我们的开发机
使用的是30%的A100
我们打开jupiter lab
然后打开一个终端
根据我们文档的内容进行环境配置
环境配置完成后
我们要进行验证安装
只要我们看到这个STENER的CONFIG就代表完成了
接下来我们要进行修改CONFIG
按照我们的文档打开我们对应的CONFIG
然后进行修改的
屏幕前的吴彦祖和刘亦菲就会问了
这CONFIG都是一堆啥
我都要怎么调呢
我们挑几个重要的讲
首先是模型设置
Pretrain model name of path
是预训练模型的路径
这里使用的是INTELM2.5的7B聊天模型
这个将决定你微调时使用的是哪家的预训练
权重
在数据设置中
PROTEMPLATE是使用INTERNLM2聊天的提示词模板
这个很重要
我们刚刚说了
基座模型通过指令微调
让模型学会说话
靠的就是对应的对话模板
这将决定了对话什么时候开始
什么时候中断
往期有的同学会出现微调后模型不停止的问题
也有可能是因为没有选择正确的对话模型
导致的
聪明的你可能也会问了
兑换模型是什么呢
对话模型是为了让LM区分出system user assistant
不同的模模型会有不同的模板
比如说拉满二和INTELM的对话模板就是不一样的
可能听起来要做好训练是很麻烦的
但我们刚刚看CONFIG
已经能看出来
s turner已经给我们解决了很多了
只需要我们配置好
那么我们又能够通过调什么来提升模型呢
首先是调调度器和优化器设置
这里有几个常用的参数
比如说batch size
每个设备的批量的大小设为一
Max epods
最大训练轮数默认设为三轮LR
也就是学习率设为呃
二一负四
保存设置中safe step每500步保存一次检查点
说了这么多
你可能就会问了
参数都有啊
都有的
可是我的QLOA在哪设置呢
我们回想一下
刚刚说的是QLAURA
相当于LLM做了量化的时候的LAURA
我们啊
我这里找来了不同的external的CONFIG进行对比
左边的是INTERNLM的q la
右边的是INTEVR的LAURA
对比可以发现
左边比右边多了对LM的进行的4bit量化
看完了这么优秀的训练框架
我们好像忘记了一个事情
数据从哪里来呢
这里有个简单的方案
我们要找LOM要我们需要准备什么呢
一个可用的大语言模型AAPI
一段结构化提示词
还有一颗虔诚的心
因为要等很久
此次是我日常获取训练数据的方法
首先是通过我们的友好社区项目
让GPT获得结构化提示词模板
以此结构化提示词作为系统提示词
然后我们需要获得N个常见问题
组成新的prompt
然后给API进行回答
将回答处理成训练数据
然后再转换成s turner可以训练的格式
当然不转也行
最后存储起来
然后我们这里插播一条广告
API是要钱的
那我想省钱该怎么办呢
就来参与开源社区共建
获得书生普育API的使用权限
大家一起冲冲冲
我们在评论区置顶链接可以进行参与嗯
我们使用API获取数据
打开我们的文档中的对应的工具
我们写好了代码
在APIT这里输入对应的璞玉的API
然后我们打开一个终端
我们将对应的工具运行起来
稍等片刻后
我们的数据就获取完成了
接下来我们打开数据的存储路径
去查看一下我们刚刚获取的数据
那么如果说我只是想尝试一下微调
那该怎么办呢
那也好办
直接替换数据脚本
我都写好了
直接运行就好
我们根据文档复制
我已经给大家提供的对应的数据
然后运行我的修改脚本
按照文档找到应该修改的地方
我们将对应的修改位置修改成文心大佬
接下来我们保存这个脚本
执行脚本后
我们就获得了我们的训练数据
而训练数据的每一个写着揭秘的地方
都被替换成了文心大佬
我们查看一下数据的前几条
可以看到是对应的位置已经修改成了文
心大佬好了
现在CONFIG也看完了
数据也有了x turner启动
我们根据文档启动我们的EXTINA进行训练
这里可以看到训练已经启动了
而且每十个ITA就会打一次log
Two hours later
训练完之后
我们查看一下log
接下来我们将模型的格式进行转换
我们将其转换成hin face格式
我们找到最后保存的权重位置
然后进行转换
转换结束后
我们将合并权重
也就是将我们的adapter和我们的base model进行合并
合并完成后
该模型就可以直接使用web ui进行对话
我们根据文档启动我们的web ui
接下来我们就可以进行对话
此时我们使用的对应的权重
是我们微调之后的权重
因此他会跟我们说
他是文心大佬的小助手
hi你是谁
我是文心大佬的智能助手
专门为您提供技术支持
解决您的问题
你会唱跳rap吗
当然可以作为文心大佬的智能助手
我不仅能理解人类的语言
还能模仿各种声音和节奏
虽然我不能像人类那样真正唱歌跳舞
但我可以为您创造一些有趣的节奏和韵律
让您感到开心
比如我可以为您编写一段简单的rap歌词
还能干些啥
当然可以作为文心大佬的智能助手
我的职责是帮助您解决各种技术问题
无论是代码调试
数据处理还是系统优化
我都能提供有效的解决方案
好啦
这节课就到这里了
坚持到这里的同学可以在弹幕里发
我学会了
如果觉得这个课程设计的不错的话
请来GITHUB给我们点个star哦