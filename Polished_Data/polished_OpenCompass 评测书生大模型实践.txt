各位小伙伴们，大家好！

今天，我们将介绍书生葡语对实战营课程中的司南评测部分。本次讲解分为两个主要部分：第一部分是司南评测的整体介绍，第二部分将进行司南代码的实际演练，手把手带大家进行实战评测。

首先，让我们从第一部分开始。那么，第一个问题便是：我们为何需要进行评测？

首先，评测体系是模型研发的指挥棒。通过评测，我们可以识别出哪些大模型的性能更为突出，同时，评测体系也决定了模型的研发方向。其次，在医疗、金融等专业领域，模型需要具备专业知识才能有效应用，因此，我们也需要聚焦这些垂直领域，评估这些领域内模型的能力。第三，随着大模型的迅猛发展，其安全问题日益突出，而人工监督耗时费力，基于模型的评测在人类无法提供有效监督时成为必由之路。最后，模型评测是产品落地的守门员，面对具体场景和应用，需基于评测结果进行模型选型和准出。

好了，我们回答了第一个问题。那么，接下来在大模型的评测中，将面临哪些挑战呢？

首先，全面性是一个挑战。由于大模型的应用场景非常广泛，且其能力迅速发展，从最初的对话到现在的智能体等，如何构建一个既能扩展又能覆盖广泛的评测体系是一个问题。

其次，可能存在数据污染的问题。例如，某些模型在评测体系上进行过训练，从而获得较高的分数，这使得其分数失去可信度，类似于在高考前刷到了真题，最终高考分数也不具备区分度。

第三，评测成本高昂。当前评测成本非常高，例如，对客观题进行评测需要大量推理成本和GPU资源，而对主观题进行人工评分，人工劳务费也相当高昂。

最后，鲁棒性问题也需解决。某些模型可能对特定提示词敏感，更换提问方式后可能无法正确回答，这也是评测需要解决的问题之一。

回到司南本身，我们如何评测大模型？首先，根据模型类型的不同，我们将评测分为不同的模型，如基座模型和对话模型。司南支持主流商业模式的API方式评测和开源模型加载权重的方式进行评测，社区用户可根据需求选择不同方法进行评测。根据模型问题是否有固定答案，我们将评测分为客观评测和主观评测，采用不同的方法。

例如，客观评测包括选择题和问答题，如询问某大模型“中国的首都在哪里”，ABCD分别对应四个选项，模型只需输出A或B，或根据回答提取选项判断是否正确。

对于主观评测，如开放性的主观问答题，不像客观评测有固定答案。例如，让大模型写一首诗，评估模型A和模型B哪首诗写得更好，这需要更复杂的判断，而非简单规则决定。因此，对于主观评测，我们采用人类评测和模型评测两种方法，更多考虑模型评测，如使用市面上的优秀gt模型（如gt4模型）对模型A和模型B进行评分。

感谢大家的聆听，希望本次讲解对大家有所帮助！
除了客观评测与主观评测之外，我们还引入了长文本评测，如“大海捞针”。这一评测方法的具体实施方式是，在一篇长篇文档中随机插入与主题无关的句子，例如在一本关于深度学习的书籍中插入一句“小明在上海人工智能实验室实习”，然后要求大模型在阅读完该书后回答小明在哪里实习的问题。若模型能正确回答“小明在上海人工智能实验室实习”，则表明该模型具备良好的记忆与理解能力。司南评测体系支持这种长文本评测方法，目前已经发展到2.0阶段。自去年5月初步开发完成后，经过多次迭代，越来越多的科研机构与司南合作。至今年1月30日，我们正式发布了司南2.0评测体系，其功能得到了显著提升与完善，这得益于社区众多小伙伴的共同努力。我们对大家的支持与贡献表示由衷的感谢，司南评测体系也因此得到了不断优化与改进。目前，司南已被众多头部大模型研究机构广泛采用，并获得了Matter官方推荐的唯一一个国产大模型评测体系称号，同时也是社区支持最为完善的评测体系。开源评测体系已包含100多个评测集和50多万道题目。接下来，我们介绍司南生态系统。我们汇集了社区的力量，建立了工具、基准与榜单三位一体的体系。我们密切关注社区需求，每月定期更新榜单，实时反映当前模型的性能优劣，并据此优化评测工具，再向社区反馈，形成良性循环。目前，我们已构建了一个中立且全面的性能榜单，涵盖100多个大模型，并定向加入评测，包括多模态榜单，除了大语言模型评测外，多模态评测也在持续更新中。除了上述基础功能外，我们已经建立了一套完整的评测工作链。例如，使用HanFace模型进行评测时，若速度较慢，我们可以支持更换推理模型以提高后端速度。我们正在构建一个更加开源、开放的基准社区，其中包含大量数据集，用户可直接通过网页查看数据集的详细介绍及各模型评测结果。接下来，我们介绍Compass Bench模型能力洞察。对于闭源评测集，在主观评测方面，我们以对战胜率进行评估，涵盖创造性语言、数学知识推理等题目；在客观评测模块，则通过选择题和填空题进行考察，包括但不限于语言知识、数学代码难题等。Compass Bench会对各项能力进行客观评分与主观评分，并以直观的图表形式展示排名。通过上述评测分析，我们可以洞察到模型的整体能力仍有较大提升空间，闭源大模型能力接近于GBD-4水平，国内模型在中文场景中具有性能优势，开源社区未来可期。复杂推理仍是短板，但在未来发展趋势分析方面，模型的尺寸将逐渐收敛，大小模型将并重发展，稠密模型与稀疏模型之争仍需探索。长文本能力日益重要，基础架构多路线并行，复杂推理与智能体应用仍需技术突破，如当前模型在多步骤复杂推理与各类交互式智能体应用中仍面临挑战。
以下是润色后的文本，保持了专业术语的准确性，使表达更加流畅自然：

在专业知识领域，普通人类标注者的准确性可能与先进的大模型相比并不具备显著优势。随着大模型能力的不断增强，使用模型进行性能评估也逐渐成为可能。

接下来，我们介绍司南板单矩阵。首先，我们介绍大模型的评测体系。司南致力于构建科学领先、公平的大模型评测体系，携手行业助力通用人工智能的发展。我们拥有开源评测榜单、闭源评测榜单、社区评测榜单以及垂类评测榜单。接下来，我们将详细介绍这些内容。

首先是Compass Academic，这一评测工具已被学术界广泛采用，能够全面反映模型的能力差异，涵盖多个能力维度，并基于数据及相关因素进行筛选。它可以进行知识理解与推理语言的评测。接着，我们详细介绍了之前提及的Compact Bench。在这一基础上，除了语言知识理解之外，还加入了数据代码和推理，进一步丰富了其基础能力。在综合能力方面，对之前的基础能力进行了更多组合，全面考察了大模型运用各种知识进行理解、分析和推理的能力。

此外，我们设有评测基准更新制度，例如每季度更新评测基准，并吸纳最新、最优秀的基准。Compass Arena是大模型的竞技场，目前支持20多个国内外主流模型同场竞技。欢迎参与投票，以反映真实用户的反馈。

接下来，我们介绍司南评测的研究成果。在论文方面，司南已发布多篇相关论文，涵盖了大模型评测的各个领域，如MM Bench视觉语言模型综合评测基准等。此外，司南在社区中具有广泛影响力，如Mass Bench多层次数学能力评测基准、Node Punch 100万进阶版大模型捞针、CL Bench代码解释器能力分析基准以及Boat Chat大模型多轮对话评测基准，这些均为司南取得的丰富学术成果。欢迎加入我们，共同探讨下一代司南评测的方向。

下一代大模型评测的发展方向包括面向GI的评测体系、大模型动态评测、自动化构建策略，以及建立复杂的智能体评测系统。我们将对模型性能进行深入分析，探索其能力的来源与泛化性，最终构建可靠的自动化主观评测。欢迎大家扫码访问我们的官网，加入GitHub的源码仓库，并关注我们的公众号，共同交流讨论。

今天的PPT部分到此结束，接下来我们将进行司南代码实战演练，探讨如何客观全面地评估大语言模型的性能与能力。这是一个重要的课题，模型评测不仅能帮助我们了解不同模型的优势与局限性，还能为模型的改进与应用提供重要的参考依据。在此背景下，我们需要一个专业的、系统的评测工具，而OpenCompass正是这样一个强大的开源工具。现在，我将带大家一起实践，使用OpenCompass评测大语言模型。

首先，我们需要准备一个开发机并配置好环境。在这里，我们准备了OpenPass的开发机。在开发机中，打开终端并执行以下命令，即可激活环境并输出目录，位于root/OpenPass下，并将OpenPass项目克隆下来。我们使用开发机有多种方式，包括Stupid Life、Terminal和VS Code。

希望这段润色后的文本能够满足您的需求，使表达更加流畅自然，同时保持了专业术语的准确性。
您可以看到，他们均使用同一台开发机，且结果一致。选择使用哪种方式主要看哪种更为方便。接下来，我们将回到如何评估大语言模型这一话题。首先，我们需考虑的是，代言模型主要有两种常见的使用方式。

第一种方式是通过API服务来使用，例如OpenAI的GPT系列模型。这些模型通常是闭源的，我们通过调用其API服务来使用。第二种方式则是下载模型权重后自行部署，例如Meta的LLaMA系列模型以及上海人工智能实验室的InternLM模型。OpenCompass作为一个单元模型的评测框架，提供了API模型、评测模式和本地模式评测这两种方式。

以下以上海人工智能实验室的InternLM模型为例，介绍如何评估API模型。评估API模型时，我们首先需要获取API密钥（API key）和API服务地址，以及模型的名称。获取这些信息有几种途径，既可通过官方地址，也可以通过第三方平台等。以官方途径为例，我们演示如何获取API密钥和API服务地址。

在官方接口文档中，我们可以看到多种调用方式。通过示例代码，我们发现API服务地址通常不显示，而模型名称则明确标注。有了这些信息后，我们可以继续配置模型。配置模型时，需编写一个模型配置文件，并将其放置在OpenCompass目录中。打开模型配置文件，即API模型的配置文件，将代码复制进去。

配置文件中的关键信息包括API服务地址、API密钥以及模型名称。API密钥通常存储在API token中，申请后需复制并使用环境变量的方式引入。以下是具体步骤：

1. 定义环境变量并按照步骤引入。
2. 执行命令以确认环境变量已成功引入。
3. 检查API密钥是否已正确复制。

接下来，我们需要配置数据集。配置数据集同样需要新建文件，并根据评测速度的需求进行适当修改，例如只选取一个样本进行快速评测。这样做的原因是官方API调用会消耗大量算力，且每分钟只能处理有限请求，导致速度较慢。CMMLU数据集包含67个子数据集，每个数据集有几十甚至上百条样本，全面评测会非常耗时，因此每个数据集仅选取一个样本，以加快评测速度。

完成这些配置后，我们就可以运行评测代码了。但在评测之前，请注意以下事项：

- 保留InternLM、Lagent、MindSearch、LLamaIndex、OpenCompass、Xtuner、Multi-agent、书生浦语、InternVL2、transformer等专业术语的原有形式。
- 若发现类似术语但书写错误，请修改为上述术语。

通过以上步骤，您可以顺利进行大语言模型的评估工作。
我们遇到了一个相对简单的问题。具体来说，我们在项目的根目录中发现了两个名为“CONFIG”的文件。一个位于根目录下，另一个则在“open compass”目录下。这是为什么呢？据官方称，OpenCompass正计划逐步废弃根目录下的“configure”文件。然而，在当前版本中，我们会首先搜索根目录下的文件并执行操作。因此，为了避免调用错误的配置数据，我们需要通过按“CTRL + P”来查找并确认是否有重复文件。可以看到在“CONFIG”目录下存在重复的文件，这些重复文件在此处被删除，以防止后续调用时使用错误的配置文件。删除重复文件后，我们将使用正确的配置文件进行调用。完成这一步骤后，我们可以通过以下命令进行执行。当然，在评测过程中，如果出现问题，例如“CHINESE”包未安装，您需要安装该包。如果该包未引入，理论上应能正常进行评测，预计运行时间为10分钟。运行10分钟后，我们将看到每个子数据集的结果，要么是100%，要么是0%，这是因为我们的评测仅使用了一个样本。在这种情况下，我们可以稍作等待，观察评测过程。现在，评测已经开始，预计需耗时10分钟。此外，对于开源模型和本地模型的评测，我们建议您参考教程的后续部分。OpenKPass在设计评测时并未区分API模型和权重模型，因此其评测方法基本一致。大家可以在后续部分仔细测试，并按照教程进行操作。

请注意，在此版本中，保留了以下专业术语的原有形式：InternLM, Lagent, MindSearch, LLamaIndex, OpenCompass, Xtuner, Multi-agent, 书生浦语, InternVL2, transformer。如果存在类似术语但书写错误的情况，请修改为上述术语。